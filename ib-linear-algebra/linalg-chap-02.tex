%!TEX root = linear-algebra.tex
\stepcounter{lecture}
\setcounter{lecture}{2}
\sektion{Endomorphisms}

\lecturemarker{8}{22 Oct}

In this chapter, unless stated otherwise, we take $V$ to be a vector space over a field $\F$, and $\alpha:V\to V$ to be a linear map.

\begin{definition}
	An \emph{endomorphism} of $V$ is a linear map from $V$ to $V$. We write $\End(V) = \Lin(V,V)$ to denote the set of endomorphisms of $V$: %
	\begin{equation*}
		\End(V) = \left\{\alpha:V\to V,\alpha \text{ linear}\right\}.
	\end{equation*}
\end{definition}

The set $\End(V)$ is an algebra: as well as being  a vector space over $\F$, we can also multiply elements of it -- if  $\alpha, \beta\in\End(V)$, then $\alpha \beta \in \End(V)$, i.e.{} product is \emph{composition} of linear maps.

Recall we have also defined
\begin{equation*}
	\GL(V) = \left\{\alpha\in\End(V):\alpha \text{ invertible}\right\}.
\end{equation*}
Fix a basis $b_1,\ldots,b_n$ of $V$ and use it as the basis for both the source and target of $\alpha:V\to V$. Then $\alpha$ defines a matrix $A\in\Mat_n(\F)$,  by $\alpha(b_j) = \sum_i a_{ij} \,b_i$. If $b\p_1,\ldots,b\p_n$ is another basis, with change of basis matrix $P$, then the matrix of $\alpha$ with respect to the new basis is $PAP^{-1}$.
\begin{equation*}
	\xymatrix{
		V
			\ar[r]^\alpha
			\ar[d]_\cong
		&
		V
			\ar[d]^\cong
		\\
		\Fn
			\ar[r]_A
		&
		\Fn
	}
\end{equation*}
Hence the properties of $\alpha:V\to V$ which don't depend on choice of basis are  the properties of the matrix $A$ which are also the properties of all \emph{conjugate} matrices $PAP^{-1}$.

These are the properties of the set of $\GL(V)$ orbits on $\End(V)=\Lin(V,V)$, where $\GL(V)$ acts on $ \End(V)$, by  $(g,\alpha)\mapsto g\alpha g^{-1}$.

In the next two chapters we will determine the set of orbits. This is called the theory of \emph{Jordan normal forms}, and is quite involved. 

Contrast this with the properties of a linear map $\alpha : V \to W$ which don't depend on the choice of basis of both $V$ and $W$; that is, the determination of the $\GL(V)\times \GL(W)$ orbits on $\Lin(V,W)$. In chapter 1, we've seen that the only property of a linear map which doesn't depend on the choices of a basis is its rank -- equivalently that the set of orbits is isomorphic to $\{i \mid 0\leq i\leq \min(\dim V,\dim W)\}$.

We begin by defining the determinant, which is a property of an endomorphism which doesn't depend on the choice of a basis.

	\pagebreak

\subsection{Determinants} % (fold)
\label{sub:determinants}

\begin{definition}
	We define the map $\det:\Mat_n(\F) \to \F$ by %
	\begin{equation*}
		\det A=\sum_{\sigma\in S_n} \epsilon(\sigma)\,a_{1,\sigma(1)}\ldots a_{n,\sigma(n)}.
	\end{equation*}

\end{definition}

Recall that $S_n$ is the group of permutations of $\{1,\ldots,n\}$.
% The elements are permutations, and the group operation is composition of permutations $(\sigma\circ\tau)(j) = \sigma(\tau(j))$. 
Any $\sigma \in S_n$ can be written as a product of transpositions $(i j)$.

Then $\epsilon:S_n\to\{\pm1\}$ is a group homomorphism taking
\begin{equation*}
	\epsilon(\sigma) =
	\begin{cases}
		+1 & \text{if number of transpositions is even,} \\ %
		-1 & \text{if number of transpositions is odd.} \\ %
	\end{cases}
\end{equation*}

In class, we had a nice interlude here on drawing pictures for symmetric group elements as braids, composition as concatenating pictures of braids, and how $\epsilon(w)$ is the parity of the number of crossings in any picture of $w$. This was just too unpleasant to type up; sorry!


\begin{example}
	We can calculate $\det$ by hand for small values of $n$: %
	\begin{align*}
		\det \mat{a_{11}}
			&= a_{11} \\[1.5pt]
		\det \mat{a_{11} & a_{12} \\ a_{21} & a_{22}}
			&= a_{11} a_{22} - a_{12} a_{21} \\[1.5pt] %
		\det \mat{a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33}}
			&=
			\begin{array}{l}
				a_{11} a_{22} a_{33}
				+ a_{12} a_{23} a_{31}
				+ a_{13} a_{21} a_{32} \\ \qquad
				- a_{13} a_{22} a_{31}
				- a_{12} a_{21} a_{33}
				- a_{11} a_{23} a_{32} %
			\end{array}
	\end{align*}
\end{example}
	The complexity of these expressions grows nastily; when calculating determinants it's usually better to use a different technique rather than directly using the definition. %


\begin{lemma}
	If $A$ is upper triangular, that is, if $a_{ij}=0$ for all $i>j$, then $\det A =a_{11}\,\ldots\,a_{nn}$. %
\end{lemma}

\begin{proof}
	From the definition of determinant:
	\begin{equation*}
		\det A = \sum_{\sigma\in S_n} \epsilon(\sigma)\,a_{1,\sigma(1)} \ldots a_{n,\sigma(n)}.
	\end{equation*}
	If a product contributes, then we must have $\sigma(i)\leq i$ for all $i=1,\ldots,n$. Hence $\sigma(1)=1$, $\sigma(2)=2$, and so on until $\sigma(n)=n$. Thus the only term that contributes is the identity, $\sigma=\id$, and $\det A = a_{11}\,\ldots\,a_{nn}$. %
\end{proof}

\begin{lemma}
	$\det A^\Trans=\det A$, where $(A^\Trans)_{ij}=A_{ji}$ is the \emph{transpose}. %
\end{lemma}

\begin{proof}
	From the definition of determinant, we have %
	\begin{align*}
		\det A^\Trans
		&= \sum_{\sigma\in S_n} \epsilon(\sigma)\,a_{\sigma(1),1} \ldots a_{\sigma(n),n} \\
		&= \sum_{\sigma\in S_n} \epsilon(\sigma) \prod_{i=1}^n a_{\sigma(i),i}
		\intertext{Now $\prod_{i=1}^n a_{\sigma(i),i} = \prod_{i=1}^n a_{i,\sigma^{-1}(i)}$, since they contain the same factors but in a different order. We relabel the indices accordingly:} %
		&= \sum_{\sigma\in S_n} \epsilon(\sigma) \prod_{k=1}^n a_{k,\sigma^{-1}(k)}
		\intertext{Now since $\epsilon$ is a group homomorphism, we have $\epsilon(\sigma\cdot\sigma^{-1}) = \epsilon(\iota) = 1$, and thus $\epsilon(\sigma) = \epsilon(\sigma^{-1})$. We also note that just as $\sigma$ runs through $\{1,\ldots,n\}$, so does $\sigma^{-1}$. We thus have} %
		&= \sum_{\sigma\in S_n} \epsilon(\sigma) \prod_{k=1}^n a_{k,\sigma(k)} = \det A. \qedhere %
	\end{align*}
\end{proof}

Writing $v_i$ for the $i$th column of $A$, we can consider $A$  as an $n$-tuple of column vectors, $A=(v_1,\ldots,v_n)$. Then $\Mat_n(\F) \cong \Fn\times\cdots\times\Fn$, and det is a function $\Fn\times\cdots\times\Fn \to \F$.

\begin{proposition}
	The function $\det:\Mat_n(\F) \to \F$ is \emph{multilinear}; that is, it is linear in each column of the matrix separately, so: \label{prop-multilinearity} %
	\begin{align*}
		\det(v_1,\ldots,\lambda_i\,v_i,\ldots,v_n)
		&= \lambda_i\,\det(v_1,\ldots,v_i,\ldots,v_n) \\
		\det(v_1,\ldots,v_i'+v_i'',\ldots,v_n)
		&= \det(v_1,\ldots,v_i',\ldots,v_n)
		 + \det(v_1,\ldots,v_i'',\ldots,v_n).
	\end{align*}
	We can combine this into the single condition
	\begin{align*}
		\det(v_1,\ldots,\lambda_i\p\,v_i\p + \lambda_i\pp\,v_i\pp,\ldots,v_n)
		&= \lambda_i\p\,\det(v_1,\ldots,v_i\p,\ldots,v_n) \\
		&\qquad + \lambda_i\pp \, \det(v_1,\ldots,v_i\pp,\ldots,v_n). %
	\end{align*}
\end{proposition}

\begin{proof}
	Immediate from the definition: $\det A$ is a sum of terms $a_{1,\sigma(1)},\ldots,a_{n,\sigma(n)}$, each of which contains only one factor  from the $i$th column: $a_{\sigma^{-1}(i),i}$. If this term is $\lambda_i'\,a_{\sigma^{-1}(i),i} + \lambda''_i\,a''_{\sigma^{-1}(i),i}$, then the determinant expands as claims. %
\end{proof}

\begin{example}
	If we split a matrix along a single column, such as below, then $\det (A)=\det A' + \det A''$. %
	\begin{equation*}
		  \det\mat{1 & 7 & 1 \\ 3 & 4 & 1\\ 2 & 3 & 0}
		= \det\mat{1 & 3 & 1 \\ 3 & 2 & 1 \\ 2 & 1 & 0}
		+ \det\mat{1 & 4 & 1 \\ 3 & 2 & 1 \\ 2 & 2 & 0}
	\end{equation*}
	Observe how the first and third columns remain the same, and only the second column changes. (Don't get confused: note that  $\det(A+B)\neq \det A + \det B$ for general $A$ and $B$.) %
\end{example}

\begin{corollary}
	$\det(\lambda A)=\lambda^n A$. %
\end{corollary}

\begin{proof}
	This follows immediately from the definition, or from applying the result of proposition~\ref{prop-multilinearity} multiple times. %
\end{proof}

\begin{proposition}
	If two columns of $A$ are the same, then $\det A=0$. %
\end{proposition}

\begin{proof}
	Suppose $v_i$ and $v_j$ are the same. Let $\tau=(i\,j)$ be the transposition in $S_n$ which swaps $i$ and $j$. Then $S_n = A_n \coprod A_n\tau$, where $A_n = \ker \epsilon : S_n \to \{\pm 1\} $. We will prove the result by splitting the sum %
	\begin{equation*}
		\det A = \sum_{\sigma\in S_n} \epsilon(\sigma) \prod_{i=1}^n a_{\sigma(i),i} 
	\end{equation*}
	into a sum over these two cosets for $A_n$, observing that for all $\sigma \in A_n$, $\epsilon(\sigma)=1$ and $\epsilon(\sigma\tau)=-1$. %

	Now, for all $\sigma \in A_n$ we have
	\begin{equation*}
		a_{1,\sigma(1)} \ldots a_{n,\sigma(n)} = a_{1,\tau\sigma(1)} \ldots a_{n,\tau\sigma(n)}, %
	\end{equation*}
	as if $\sigma(k)\not\in\{i,j\}$, then $\tau\sigma(k) = \sigma(k)$,  and if $\sigma(k) = i$, then %
	\begin{equation*}
		a_{k,\tau\sigma(k)} = a_{k,\tau(i)} = a_{k,j} = a_{k,i} = a_{k,\sigma(k)},
	\end{equation*}
	and similarly  if $\sigma(k)=j$. Hence
	\begin{equation*}
		\det A = \sum_{\sigma\in A_n} \left( \prod_{i=1}^n a_{\sigma(i),i} - \prod_{i=1}^n a_{\sigma\tau(i),i} \right) = 0. \qedhere %
	\end{equation*}
%	\begin{align*}
%		& a_{\sigma(1),1} \ldots a_{\sigma(k),k} \ldots a_{\sigma(l),l} \ldots a_{\sigma(n),n} %
%		- & a_{\sigma(1),1} \ldots a_{\sigma(l),k} \ldots a_{\sigma(k),l} \ldots a_{\sigma(n),n} = 0, %
%	\end{align*}
%	and a similar calculation holds for $\sigma\in S_n\backslash A_n$.\todo{Find that notation with the double perp, and note that $S_n=A_n+\tau A_n$. etc.} %
\end{proof}

\begin{proposition}
	If $I$ is the identity matrix, then $\det I=1$ %
\end{proposition}

\begin{proof*}
	Immediate. %
\end{proof*}

\begin{theorem}
	These three properties characterise the function  $\det$. %
\end{theorem}

Before proving this, we need some language.

\begin{definition}
	A function $f:\Fn\times\cdots\times\Fn\to\F$ is a \emph{volume form} on $\Fn$ if
	\begin{enumerate}
		\item It is \emph{multilinear}, that is, if
		\begin{align*}
			f(v_1,\ldots,\lambda_i\,v_i,\ldots,v_n)
			&= \lambda_i\,f(v_1,\ldots,v_i,\ldots,v_n) \\
			f(v_1,\ldots,v_i+v_i\p,\ldots,v_n)
			&= f(v_1,\ldots,v_i,\ldots,v_n)
			 + f(v_1,\ldots,v_i\p,\ldots,v_n).
		\end{align*}
		We saw earlier that we can write this in a single condition:
		\begin{align*}
			f(v_1,\ldots,\lambda_i\,v_i + \lambda_i\p\,v_i\p,\ldots,v_n)
			&= \lambda_i\,f(v_1,\ldots,v_i,\ldots,v_n) \\
			&\qquad + \lambda_i\p \, f(v_1,\ldots,v_i\p,\ldots,v_n). %
		\end{align*}
		
		\item It is \emph{alternating}; that is, whenever $i\neq j$ and $v_i=v_j$, then $f(v_1,\ldots,v_n)=0$. %
	\end{enumerate}
\end{definition}

\begin{example}
	We have seen that $\det:\Fn\times\cdots\times\Fn \to \F$ is a volume form. It is a volume form $f$ with $f(e_1,\ldots,e_n)=1$ (that is, $\det I=1$). %
\end{example}

\begin{remark}
	Let's explain the name ``volume form''.

	Let $\F = \R$, and consider the volume of a rectangular box with a corner at $0$ and sides defined by $v_1, \dots, v_n$ in $\R^n$. The volume of this box is a function of $v_1,\dots,v_n$ that almost satisfies the properties above. It doesnt quite satisfy linearity, as the volume of a box with sides defined by $-v_1,v_2,\dots,v_n$ is the same as that of the box with sides defined by $v_1, \dots, v_n$, but this is the only problem. (Exercise: check that the other properties of a volume form are immediate for voluems of rectangular boxes.)

	You should think of this as saying that a volume form gives a \emph{signed} version of the volume of a rectangular box (and the actual volume is the absoulute value). In any case, this explains the name. You've also seen this in multi-variable calculus, in the way that the determinant enters into the formula for what happens to integrals when you change coordinates.
\end{remark}

\begin{theorem}
	[Precise form] The set of volume forms forms a vector space of dimension 1. This line is called the \emph{determinant line}. %
\end{theorem}

\begin{proof*}
	\lecturemarker{9}{24 Oct}
	It is immediate from the definition that volume forms are a vector space. Let $e_1,\ldots,e_n$ be a basis of $V$ with $n=\dim V$. Every element of $V^n$ is of the form %
	\begin{equation*}
		\left( \sum a_{i1} \,e_i, \sum a_{i2}\,e_i, \ldots, \sum a_{in}\,e_i \right),
	\end{equation*}
	with $a_{ij}\iF$ (that is, we have an isomorphism of sets $V^n \isomto \Mat_n(\F)$). So if $f$ is a volume form, then %
	\begin{align*}
		   f\left( \sum_{i_1=1}^n a_{i_1 1}\, e_{i_1}, \ldots, \sum_{i_n=1}^n a_{i_n n}\,e_{i_n} \right) %
		&= \sum_{i_1=1}^n a_{i_1 1} \,f\left( e_{i_1}, \sum_{i_2=1}^n a_{i_2 1}\, e_{i_2}, \ldots, \sum_{i_n=1}^n a_{i_n n}\,e_{i_n} \right) \\ %
		&= \cdots = \sum_{1\leq i_1,\ldots,i_n\leq n} a_{i_1 1} \ldots a_{i_n n} \, f(e_{i_1},\ldots,e_{i_n}), %
	\end{align*}
	by linearity in each variable. But as $f$ is alternating, $f(e_{i_1},\ldots,e_{i_n})=0$ unless $i_1,\ldots,i_n$ is $1,\ldots,n$ in some order; that is, %
	\begin{equation*}
		\left( i_1,\ldots,i_n \right) = \left( \sigma(1),\ldots,\sigma(n) \right)
	\end{equation*}
	for some $\sigma\in S_n$.
	
%		\pagebreak
	
	\textbf{Claim.} $f(e_{\sigma(1)},\ldots,e_{\sigma(n)})=\epsilon(\sigma) \,f(e_1,\ldots,e_n)$. %
	
	Given the claim, we get that the sum above simplifies to
	\begin{equation*}
		\sum_{\sigma\in S_n} a_{\sigma(1),1} \ldots a_{\sigma(n),n} \, \epsilon(w)\,f(e_1,\ldots,e_n), %
	\end{equation*}
	and so the volume form is determined by $f(e_1,\ldots,e_n)$; that is, $\dim(\{\text{vol forms}\})\leq 1$. But $\det:\Mat_n(\F) \to \F$ is a well-defined non-zero volume form, so we must have $\dim(\{\text{vol forms}\}) = 1$. %
\end{proof*}

Note that we have just shown that for any volume form
\begin{equation*}
	f(v_1,\ldots,v_n) = \det(v_1,\ldots,v_n)\,f(e_1,\ldots,e_n).
\end{equation*}
So to finish our proof,  we just have to prove our claim.

\begin{proof}
	[Proof of claim] First, for any $v_1,\ldots,v_n\in V$, we show that %
	\begin{equation*}
		f(\ldots, v_i, \ldots, v_j, \ldots)
		= -f(\ldots,v_j,\ldots,v_i,\ldots),
	\end{equation*}
	that is, swapping the $i$th and $j$th entries changes the sign. Applying multilinearity is enough to see this: %
	\begin{align*}
		   \underset{=0 \text{ as alternating}}{f(\ldots,v_i+v_j,\ldots,v_i+v_j,\ldots)} %
		&= \underset{=0 \text{ as alternating}}{f(\ldots,v_i,\ldots,v_i,\ldots)}
		 + \underset{=0 \text{ as alternating}}{f(\ldots,v_j,\ldots,v_j,\ldots)} \\
		&\qquad
		 + f(\ldots,v_i,\ldots,v_j,\ldots)
		 + f(\ldots,v_j,\ldots,v_i,\ldots).
	\end{align*}
	Now the claim follows, as an arbitrary permutation can be written as a product of transpositions, and $\epsilon(w)=\left( -1 \right)^{\#\text{ of transpositions}}$. %
\end{proof}

\begin{remark}
	Notice that if $\Z/2\not\subset\F$ is not a subfield (that is, if $1+1\neq 0$), then for a multilinear form $f(x,y)$ to be alternating, it suffices that $f(x,y)=-f(y,x)$. This is because we have $f(x,x)=-f(x,x)$, so $2f(x,x)=0$, but $2\neq 0$ and so $2^{-1}$ exists, giving $f(x,x)=0$. If $2=0$, then $f(x,y)=-f(y,x)$ for any $f$ and the correct definition of alternating is $f(x,x)=0$. %

	\emph{If that didn't make too much sense, don't worry: this is included for mathematical interest, and isn't essential to understand anything else in the course.} %
\end{remark}

\begin{remark}
	If $\sigma\in S_n$, then we can attach to it a matrix $P(\sigma)\in\GL_n$ by %
	\begin{equation*}
		P(\sigma)_{ij} =
		\begin{cases}
			1 & \text{if } \sigma^{-1} i = j, \\ %
			0 & \text{otherwise}.
		\end{cases}
	\end{equation*}
\end{remark}

\begin{exercises}
Show that:
\begin{enumerate}
	\shortskip
	\item $P(w)$ has exactly one non-zero entry in each row and column, and that entry is a 1. Such a matrix is called a \emph{permutation matrix}. %
	\item $P(w)\,e_i = e_j$, hence
	\item $P:S_n \to \GL_n$ is a group homomorphism;
	\item $\epsilon(w)=\det P(w)$.
\end{enumerate}
\end{exercises}

\vspace{3pt}

% Why is it called a volume form? DIAGRAMS DIAGRAMS DIAGRAMS

%	\pagebreak

\begin{theorem}
	Let $A,B\in\Mat_n(\F)$. Then $\det AB=\det A \det B$. %
\end{theorem}

\begin{proof}
	[Slick proof] Fix $A\in\Mat_n(\F)$, and consider $f:\Mat_n(\F)\to\F$ taking $f(B) = \det AB$. We observe that $f$ is a volume form. (Exercise: check this!!) But then %
	\begin{equation*}
		f(B) = \det B \cdot f(e_1,\ldots,e_n).
	\end{equation*}
	But by the definition,
	\begin{equation*}
		f(e_1,\ldots,e_n) = f(I) = \det A. \qedhere
	\end{equation*}
\end{proof}

\begin{corollary}
	If $A\in\Mat_n(\F)$ is invertible, then $\det A^{-1}=1/\det A$.
\end{corollary}
\vspace{-6pt}
\begin{proof}
	Since $AA^{-1}=I$, we have %
	\begin{equation*}
		\det A \det A^{-1} = \det {AA^{-1}} = \det I = 1,
	\end{equation*}
	by the theorem, and rearranging gives the result.
\end{proof}

\begin{corollary}
	If $P\in\GL_n(\F)$, then
	\begin{equation*}
		\det(PAP^{-1})=\det P \det A \det P^{-1} = \det A.
	\end{equation*}
\end{corollary}
\vspace{-9pt}
\begin{definition}
	Let $\alpha:V\to V$ be a linear map. Define $\det\alpha\iF$ as follows: choose \emph{any} basis $b_1,\ldots,b_n$ of $V$, and let $A$ be the matrix of $\alpha$ with respect to the basis. Set $\det\alpha=\det A$, which is well-defined by the corollary. %
\end{definition}
\vspace{3pt}

\begin{remark}
	Here is a coordinate free definition of $\det\alpha$. %
	
	Pick $f$ any volume form for $V$, $f\neq 0$. Then
	\begin{equation*}
		\left( x_1,\ldots,x_n \right) \mapsto f(\alpha x_1,\ldots,\alpha x_n) = (f\alpha)(x_1,\ldots,x_n) %
	\end{equation*}
	is also a volume form. But the space of volume forms is one-dimensional, so there is some $\lambda\iF$ with $f\alpha=\lambda f$, and we define %
	\begin{equation*}
		\lambda=\det\alpha
	\end{equation*}
	(Though this definition is independent of a basis, we haven't gained much, as we needed to choose a basis to say anything about it.) %
\end{remark}

\begin{proof}
	[Proof 2 of $\det AB=\det A \det B$] We first observe that it's true if $B$ is an elementary column operation; that is, $B=I+\alpha E_{ij}$. Then $\det B=1$. But %
	\begin{equation*}
		\det AB = \det A + \det A',
	\end{equation*}
	where $A'$ is $A$ except that the $i$th and $j$th column of $A'$ are the same as the $j$th column of $A$. But then $\det A'=0$ as two columns are the same. %
	
	Next, if $B$ is the permutation matrix $P((i\;j)) = s_{ij}$, that is, the matrix obtained from the identity matrix by swapping the $i$th and $j$th columns, then $\det B=-1$, but $A\,s_{ij}$ is $A$ with its $i$th and $j$th columns swapped, so $\det AB = \det A \det B$. %
	
	Finally, if $B$ is a matrix of zeroes with $r$ ones along the leading diagonal, then if $r=n$, then $B=I$ and $\det B=1$. If $r<n$, then $\det B=0$. But then if $r<n$, $AB$ has some columns which are zero, so $\det AB=0$, and so the theorem is true for these $B$ also. %
	
	Now any $B\in\Mat_n(\F)$ can be written as a product of these three types of matrices. So if $B=X_1 \cdots X_r$ is a product of these three types of matrices, then %
	\begin{align*}
		   \det AB
		&= \det \left(( AX_1 \cdots X_{m-1} \right) X_m ) \\
		&= \det (A X_1 \cdots X_{m-1}) \det X_m  \\
		&= \cdots
		 = \det A \det X_1 \cdots \det X_m \\
		&= \cdots = \det A \det \left( X_1 \cdots X_m \right) \\
		&= \det A \det B. \qedhere
	\end{align*}
\end{proof}

\begin{remark}
	That determinants behave well with respect to row and column operations is also a useful way for humans (as opposed to machines!) to compute determinants. %
\end{remark}

%\begin{corollary}
%	$\det PAP^{-1}=\det A$, so we can define $\det \alpha$, $\alpha:V\to V$, for any choice of basis $b_1,\ldots,b_n$ of $V$ to the matrix $A$ for $\alpha$. %
%\end{corollary}

\begin{proposition}
	Let $A\in \Mat_n(\F)$. Then the following are equivalent: %
	\begin{enumerate}
		\shortskip
		\item $A$ is invertible;
		\item $\det A\neq 0$;
		\item $r(A)=n$.
	\end{enumerate}
\end{proposition}

\begin{proof}
	(i) $\implies$ (ii). Follows since $\det A^{-1}=1/\det A$. %
	
	(iii) $\implies$ (i). From the rank-nullity theorem, we have
	\begin{equation*}
		r(A)=n \iff \ker\alpha=\{0\} \iff A \text{ invertible}.
	\end{equation*}
	Finally we must show (ii) $\implies$ (iii). If $r(A)<n$, then $\ker\alpha\neq \{0\}$, so there is some $\Lambda=\left( \lambda_1,\ldots,\lambda_n \right)^\Trans \in\Fn$ such that $A\Lambda=0$, and $\lambda_k\neq 0$ for some $k$. Now put %
	\begin{equation*}
		B = \mat{
			1 & & & \lambda_1 \\
			& 1 & & \lambda_2 \\
			& & \ddots & \vdots \\
			& & & \lambda_k \\
			& & & \vdots & \ddots \\
			& & & \lambda_n & & 1
		}
	\end{equation*}
%	Replace the $k$th column of $I$ by $\Lambda$. 
Then $\det B=\lambda_k\neq 0$, but $AB$ is a matrix whose $k$th column is 0, so $\det AB=0$; that is, $\det A=0$, since $\lambda_k\neq 0$. %
\end{proof}

This is a horrible and unenlightening proof that $\det A\neq 0$ implies the existence of $A^{-1}$. A good proof would write the matrix coefficients of $A^{-1}$ in terms of $\left( \det A \right)^{-1}$ and the matrix coefficients of $A$. We will now do this, after some showing some further properties of the determinant.

We can compute $\det A$ by expanding along any column or row.
\lecturemarker{10}{26 Oct}

\begin{definition}
	Let $A^{ij}$ be the matrix obtained from $A$ by deleting the $i$th row and the $j$th column. %
\end{definition}

\begin{theorem}
	\begin{enumerate}
		\item Expand along the $j$th column:
		\begin{align*}
			   \det A
			&= \left( -1 \right)^{j+1} a_{1j} \det A^{1j}
			 + \left( -1 \right)^{j+2} a_{2j} \det A^{2j}
			 + \cdots
			 + \left( -1 \right)^{j+n} a_{nj} \det A^{nj} \\
			&= \left( -1 \right)^{j+1} \left[ a_{1j} \det A^{1j} - a_{2j} \det A^{2j} + a_{3j} \det A^{3j} - \cdots +\left( -1 \right)^{n+1} a_{nj} \det A^{nj} \right] %
		\end{align*}
(the thing to observe here is that the signs alternate!)
		\item Expanding along the $i$th row:
		\begin{equation*}
			\det A = \sum_{j=1}^n \left( -1 \right)^{i+j} a_{ij} \det A^{ij}.
		\end{equation*}
	\end{enumerate}%
\end{theorem}

The proof is boring book keeping.

\begin{proof} Put in the definition of $A^{ij}$ as a sum over $w\in S_{n-1}$, and expand. We can tidy this up slightly, by writing it as follows: write $A=\left( v_1\,\cdots\,v_n \right)$, so  $ v_j = \sum_i a_{ij} \, e_i$. Then
	\begin{align*}
	 	\det A
		 = \det(v_1,\ldots,v_n)
		&= \sum_{i=1}^n a_{ij} \det(v_1,\ldots,v_{j-1},e_i,v_{j+1},\ldots,v_n) \\
		&= \sum_{i=1}^n \left( -1 \right)^{j-1} a_{ij} \det\left( e_i,v_1,\ldots,v_{j-1},v_{j+1}, \ldots, v_n \right). %
	\end{align*}
	as $\epsilon(1 \,2\,\ldots\,j) = (-1)^{j-1}$ (in class we drew a picture of this symmetric group element, and observed it had $j-1$ crossings.) Now $e_i=(0,\ldots,0,1,0,\ldots,0)^\Trans$, so we pick up $\left( -1 \right)^{i-1}$ as the sign of the permutation $(1 \,2\,\ldots\,i )$ that rotates the 1st through $i$th rows, and so we get
	\begin{equation*}
		\sum_i \left( -1 \right)^{i+j-2} a_{ij} \det\mat{1 & * \\ 0 & A^{ij}} = \sum_i \left( -1 \right)^{i+j} a_{ij} \det A^{ij}. \qedhere %
	\end{equation*}
\end{proof}

\begin{definition}
	For $A\in\Mat_n(\F)$, the \wiki{Adjugate_matrix}{adjugate matrix}, denoted by $\adj A$, is the matrix with %
	\begin{equation*}
		\left( \adj A \right)_{ij} = \left( -1 \right)^{i+j} \det A^{ji}.
	\end{equation*}
\end{definition}

\begin{example}
	\begin{align*}
		\adj\mat{a & b \\ c & d}
		&= \mat{d & -b \\ -c & a}, \qquad\quad \adj\mat{1 & 1 & 2 \\ 0 & 2 & 1 \\ 1 & 0 & 2} 
		&= \mat{4 & -2 & -3 \\ 1 & 0 & -1 \\ -2 & 1 & 2}.
	\end{align*}
\end{example}

\bigskip


\begin{theorem}
	[\wikirm{Cramer's_rule}{Cramer's rule}] $\left( \adj A \right) \cdot A = A \cdot \left( \adj A \right) = \left( \det A \right) \cdot I$. %
\end{theorem}

\begin{proof}
	We have %
	\begin{equation*}
		  \left( \left( \adj A \right) A \right)_{jk}
		= \sum_{i=1}^n \left( \adj A \right)_{ji} a_{ik}
		= \sum_{i=1}^n \left( -1 \right)^{i+j} \det A^{ij} a_{ik}
	\end{equation*}
	Now, if we have a diagonal entry $j=k$  then this is exactly the formula for $\det A$ in (i) above. If $j\neq k$, then by the same formula, this is $\det A\p$, where $A\p$ is obtained from $A$ by replacing its $j$th column with the $k$th column of $A$; that is $A\p$ has the $j$ and $k$th columns the \emph{same}, so $\det A\p=0$, and so this term is zero.%
\end{proof}

\begin{corollary}
	$A^{-1}=\df{1}{\det A}  \adj A$ if $\det A\neq 0$. %
\end{corollary}
\vspace{-6pt}

The proof of Cramer's rule only involved multiplying and adding, and the fact that they satisfy the usual distributive rules and that multiplication and addition are commutative. A set in which you can do this is called a \emph{commutative ring}. Examples include the integers $\Z$, or polynomials $\F[x]$.

So we've shown that if $A\in\Mat_n(R)$, where $R$ is any commutative ring, then there exists an inverse  $A^{-1}\in\Mat_n(R)$ if and only if $\det A$ has an inverse in $R$: $\left( \det A \right)^{-1}\in R$. For example, an integer matrix $A\in\Mat_n(\Z)$ has an inverse with integer coefficients if and only if $\det A=\pm 1$.

Moreover, the matrix coefficients of $\adj A$ are polynomials in the matrix coefficients of $A$, so the matrix coefficients of $A^{-1}$ are polynomials in the matrix coefficients of $A$ and the inverse of the function $\det A$ (which is itself a polynomial function of the matrix coefficients of $A$).

That's very nice to know.

% subsection determinants (end)
