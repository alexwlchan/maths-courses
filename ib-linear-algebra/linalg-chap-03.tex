%!TEX root = linear-algebra.tex
\stepcounter{lecture}
\setcounter{lecture}{3}
\sektion{Jordan normal form}

In this chapter, unless stated otherwise, we take $V$ to be a finite dimensional vector space over a field $\F$, and $\alpha:V\to V$ to be a linear map. We're going to look at what matrices look like up to conjugacy; that is, what the map $\alpha$ looks like, given the freedom to choose a basis for $V$.

% This is what matrices look like up to conjugacy; that is, what $\alpha:V\to V$ looks like, given the freedom to choose a basis for $V$.
% 
% \begin{example}
% 	Consider $V=\Fn$, $A\in\Mat_n(\F)$, which defines a linear map $x\mapsto Ax$.
% \end{example}

\subsection{Eigenvectors and eigenvalues} % (fold)
\label{sub:eigenvectors_and_eigenvalues}

\begin{definition}
	A non-zero vector $v\in V$ is an \emph{eigenvector} for $\alpha:V\to V$ if $\alpha(v)=\lambda v$, for some $\lambda\iF$. Then $\lambda$ is called the \emph{eigenvalue} associated with $v$, and the set
	\begin{equation*}
		V_\lambda = \left\{v\in V:\alpha(v)=\lambda v\right\}
	\end{equation*}
	is called the \emph{eigenspace of $\lambda$ for $\alpha$}, which is a subspace of $V$.
\end{definition}

We observe that if $\iota : V \to V$ is the identity map, then
\begin{equation*}
	V_\lambda = \ker(\lambda \iota-\alpha:V\to V).
\end{equation*}
So if $v\in V_\lambda$, then we can choose $v$ to be a non-zero vector if and only if $\ker(\lambda \iota-\alpha)\neq \{0\}$, which is equivalent to saying that $\lambda \iota-\alpha$ is \emph{not} invertible. Thus
\begin{equation*}
	\det(\lambda \iota-\alpha) = 0,
\end{equation*}
by the results of the previous chapter.

\begin{definition}
	If $b_1,\ldots,b_n$ is a basis of $V$, and $A\in\Mat_n(\F)$ is a matrix of $\alpha$, then
	\begin{equation*}
		\chi_\alpha(x) = \det(x\iota-\alpha) = \det(xI-A)
	\end{equation*}
	is the \emph{characteristic polynomial} of $\alpha$.
\end{definition}

The following properties follow from the definition:
\begin{enumerate}
	\item The general form is
	\begin{equation*}
		  \chi_\alpha(x)
		= \chi_A(x)
		= \det \mat{x-a_{11} & -a_{12} & \cdots & -a_{1n} \\
		  -a_{21} & x-a_{22} & \ddots & \vdots \\
		  \vdots & \ddots & \ddots & \vdots \\
		  -a_{n1} & \cdots & \cdots & x-a_{nn}} \in F[x].
	\end{equation*}
	Observe that $\chi_A(x)\in F[x]$ is a polynomial in $x$, equal to $x^n$ plus terms of smaller degree, and the coefficients
	% of $\chi_\alpha(x)$
	are polynomials in the matrix coefficients $a_{ij}$.
	
	\lecturemarker{11}{29 Oct}

	For example, if $A = \mat{a_{11} & a_{12} \\ a_{21} & a_{22}}$ then
	\begin{align*}
		\chi_A(x)
		&= x^2 - x \left( a_{11}+a_{22} \right) + (a_{11}a_{22}-a_{12}a_{21}) \\
		&= x^2 - x.\Tr A + \det A.
	\end{align*}
	
	\item Conjugate matrices have the same characteristic polynomials. Explicitly:
	\begin{align*}
		  \chi_{PAP^{-1}}(x)
		&= \det( xI - PAP^{-1}) \\
		&= \det( P\left( xI-A \right)P^{-1}) \\
		&= \det(xI-A) \\
		&= \chi_A(x).
	\end{align*}
	
	\item For $\lambda\iF$, $\chi_\alpha(\lambda) = 0$ if and only if $V_\lambda = \left\{v\in V: \alpha(v)=\lambda v\right\} \neq \{0\}$; that is, if $\lambda$ is an eigenvalue of $\alpha$. This gives us a way to find the eigenvalues of a linear map.
\end{enumerate}

\begin{example}
	If $A$ is upper-triangular with $a_{ii}$ in the $i$th diagonal entry, then
	\begin{equation*}
		\chi_A(x) = \displaystyle\prod_{i=1}^n \left( x-a_{ii} \right).
	\end{equation*}
	It follows that the diagonal terms of an upper triangular matrix are its eigenvalues.
\end{example}

\begin{definition}
	We say that $\chi_A(x)$ \emph{factors} if it factors into linear factors; that is, if
	\begin{equation*}
		\chi_A(x) = \displaystyle\prod_{i=1}^r \left( x-\lambda_i \right)^{n_i},
	\end{equation*}
	for some $n_i\iN$, $\lambda_i\iF$ and $\lambda_i\neq \lambda_j$ for $i\neq j$.
\end{definition}

\begin{examples}
	If we take $\F=\C$, then the fundamental theorem of algebra says that every polynomial $f\in\C[x]$ factors into linear terms.
	
	In $\R$, consider the rotation matrix
	\begin{equation*}
		A=\mat{\cos\theta & \sin\theta \\ -\sin\theta & \cos\theta},
	\end{equation*}
	which has characteristic polynomial
	\begin{equation*}
		\chi_A(x) = x^2-2x\cos\theta+1,
	\end{equation*}
	which factors if and only if $A=\pm I$ and $\theta=0$ or $\pi$.
\end{examples}

\begin{definition}
	If $\F$ is any field, then there is some bigger field $\overline{\F}$, the \emph{algebraic closure of $\F$}, such that $\F \subseteq \overline{\F}$ and every polynomial in $\overline{\F}[x]$ factors into linear factors. This is proved in the Part~II course \emph{Galois Theory}.
\end{definition}

\begin{theorem}
	If $A$ is an $n\times n$ matrix over $\F$, then	$\chi_A(x)$ factors if and only if $A$  is conjugate to an upper triangular matrix.
	
	In particular, this means that if $\F=\overline{\F}$, such as $\F=\C$, then every matrix is conjugate to an upper triangular matrix.
\end{theorem}

\vspace{3pt}

We can give a coordinate free formulation of the theorem: if $\alpha:V\to V$ is a linear map, then $\chi_\alpha(x)$ factors if and only if there is some basis $b_1,\ldots,b_n$ of $V$ such that the matrix of $\alpha$ with respect to the basis is upper triangular.

\begin{proof}
	($\Leftarrow$) If $A$ is upper triangular, then $\chi_A(x) = \prod (x-a_{ii})$, so done.
	
	($\Rightarrow$) Otherwise, set $V = \F^n$, and $\alpha (x) = Ax$. We induct on $\dim V$. If $\dim V=n=1$, then we have nothing to prove.
	
	As $\chi_\alpha(x)$ factors, there is some $\lambda\iF$ such that $\chi_\alpha(\lambda)=0$, so there is some $\lambda\iF$ with a non-zero eigenvector $b_1$. Extend this to a basis $b_1, \dots, b_n$ of $V$. 
	
	Now conjugate $A$ by the change of basis matrix.  (In other words, write the linear map $\alpha$, $x \mapsto Ax$ with respect to this basis $b_i$ rather than the standard basis $e_i$).

	%With respect to this basis, the linear map $\alpha:V\to V$, $\alpha(A)=\widetilde{A}$ can be made to look like a matrix 
	%with $a_{11}=\lambda$ and zeros in the rest of the first column, and an $(n-1)\times(n-1)$ matrix $A'$ in the lower right hand corner. Then

	We get a new matrix
	\begin{equation*}
		\widetilde{A} =  \mat{\lambda & * \\ 0 & A\p},
	\end{equation*}
	and it has characteristic polynomial 
	\begin{equation*}
		  \chi_{\widetilde{A}}(x)
		= \left( x-\lambda \right) \chi_{A'}(x).
	\end{equation*}
	So if $\chi_\alpha(x)$ factors, then so does $\chi_{A'}(x)$.
	
	Now, by induction, there is some matrix $P\in\GL_{n-1}(\F)$ such that $PA'P^{-1}$ is upper triangular. But now
	\begin{equation*}
		  \mat{1 & \\  & P} \widetilde{A} \mat{1 & \\  & P^{-1}}
		= \mat{\lambda & \\  & P{A\p}P^{-1}},
	\end{equation*}
	proving the theorem.
\end{proof}

\emph{Aside:} what is the meaning of the matrix $A\p$? We can ask this question more generally. Let $\alpha:V\to V$ be linear, and $W\leq V$ a subspace. Choose a basis $b_1,\ldots,b_r$ of $W$, extend it to be a basis of $V$ (add $b_{r+1},\ldots,b_n$).

Then $\alpha(W)\subseteq W$ if and only if the matrix of $\alpha$ with respect to this basis looks like
\begin{equation*}
	\mat{X & Z \\ 0 & Y},
\end{equation*}
where $X$ is $r\times r$ and $Y$ is $\left( n-r \right)\times\left( n-r \right)$, and it is clear that $\eval[1]{\alpha}_W:W\to W$ has matrix $X$, with respect to a basis $b_1,\ldots,b_r$ of $W$.

Then our question is: What is the meaning of the matrix $Y$?

The answer requires a new concept, the \emph{quotient vector space}.

\begin{exercise}
	Consider $V$ as an abelian group, and consider the coset group $V/W=\left\{v+W:v\in V\right\}$. Show that this is a vector space, that $b_{r+1}+W,\ldots,b_n+W$ is a basis for it, and $\alpha:V\to V$ induces a linear map $\widetilde{\alpha}:V/W \to V/W$ by $\widetilde{\alpha}(v+W) = \alpha(v)+W$ (you need to check this is well-defined and linear), and that with respect to this basis, $Y$ is the matrix of $\widetilde{\alpha}$.
\end{exercise}

\begin{remark}
	Let $V=W\p\oplus W\pp$; that is, $W\p\cap W\pp = \{0\}$, $W\p+W\pp=V$, and suppose that  $\alpha(W\p) \subseteq W\p$ and $\alpha(W\pp)\subseteq W\pp$. We write this as $\alpha=\alpha'\oplus\alpha\pp$, where $\alpha':W\p\to W\p$, $\alpha\pp:W\pp\to W\pp$ are the restrictions of $\alpha$. 

	In this special case the matrix of $\alpha$ looks even more special then the above for any basis $b_1,\ldots,b_r$ of $W\p$ and $b_{r+1},\ldots,b_n$ of $W\pp$ -- we have $Z=0$ also.
\end{remark}

\begin{definition}
	The \emph{trace} of a matrix $A=(a_{ij})$, denoted $\Tr(A)$, is given by
	\begin{equation*}
		\Tr(A)=\sum_i a_{ii},
	\end{equation*}
\end{definition}

\begin{lemma}
	$\Tr(AB)=\Tr(BA)$.
\end{lemma}

\begin{proof}
	$\Tr(AB) = \sum_i (AB)_{ii} = \sum_{i,j} a_{ij} b_{ji} = \sum_j (BA)_{jj} = \Tr(BA)$.
\end{proof}
\begin{corollary}
	$\Tr(PAP^{-1}) = \Tr(P^{-1} PA) = \Tr(A)$.
\end{corollary}
\vspace{-6pt}
So we define, if $\alpha:V\to V$ is linear, $\Tr(\alpha)=\Tr(A)$, where $A$ is a matrix of $\alpha$ with respect to some basis $b_1,\ldots,b_n$, and  this doesn't depend on the choice of a basis.

\begin{proposition}
	If $\chi_\alpha(x)$ factors as $\left( x-\lambda_1 \right) \cdots \left( x-\lambda_n \right)$ (repetition allowed), then
	\begin{enumerate}
		\shortskip
		\item $\Tr \alpha = \sum_i \lambda_i$;
		\item $\det\alpha = \prod_i \lambda_i$.
	\end{enumerate}
\end{proposition}

\begin{proof}
	As $\chi_\alpha$ factors, there is some basis $b_1,\ldots,b_n$ of $V$ such that the matrix of $A$ is upper triangular, the diagonal entries are $\lambda_1,\ldots,\lambda_n$, and we're done.
\end{proof}

\begin{remark}
	This is true whatever $\F$ is. Embed $\F\subseteq\overline{\F}$ (for example, $\R\subseteq\C$), and $\chi_A$ factors as $\left( x-\lambda_1 \right) \cdots \left( x-\lambda_n \right)$. Now $\lambda_1,\ldots,\lambda_n \in \overline{\F}$, not necessarily in $\F$.
	
	Regard $A\in\Mat_n(\overline{\F})$, which doesn't change $\Tr A$ or $\det A$, and we get the same result. Note that $\sum_i \lambda_i$ and $\prod_i \lambda_i$ are in $\F$ even though $\lambda_i\not\iF$.
\end{remark}

\begin{example}
	Take $A=\mat{\cos\theta & \sin\theta \\ -\sin\theta & \cos\theta}$. Eigenvalues are $e^{i\theta}$, $e^{-i\theta}$, so
	\begin{equation*}
		\Tr A = e^{i\theta}+e^{-i\theta} = 2\cos\theta
		\qquad
		\det A = e^{i\theta} \cdot e^{-i\theta} = 1.
	\end{equation*}
\end{example}

	Note that
	\begin{align*}
		   \chi_A(x)
		&= \left( x-\lambda_1 \right) \cdots \left( x-\lambda_n \right) \\
		&= x^n - \left( \sum_i \lambda_i \right) x^{n-1} + \left( \sum_{i<j} \lambda_i\,\lambda_j \right) x^{n-2} - \cdots + \left( -1 \right)^n \left( \lambda_1\ldots\lambda_n \right) \\
		&= x^n - \Tr(A) x^{n-1} + e_2 x^{n-2} - \cdots  + \left( -1 \right)^{n-1} e_{n-1} + \left( -1 \right)^n \det A,
	\end{align*}
	where the coefficients $e_1=\Tr A$, $e_2,\ldots,e_{n-1}$, $e_n=\det A$ are functions of $\lambda_1,\ldots,\lambda_n$ called \emph{elementary symmetric functions} (which we see more of in \emph{Galois theory}).
	
	Each of these $e_i$ depend only on the conjugacy class of $A$, as
	\begin{equation*}
		\chi_{PAP^{-1}}(x) = \chi_A(x).
	\end{equation*}

\bigskip

Note that $A$ and $B$ conjugate implies $\chi_A(x)=\chi_B(x)$, but the converse is false. Consider, for example, the matrices
\begin{equation*}
	A = \mat{0 & 0 \\ 0 & 0}
	\qquad
	B = \mat{0 & 1 \\ 0 & 0},
\end{equation*}
and $\chi_A(x) = \chi_B(x) =x^2$, but $A$ and $B$ are not conjugate.

For an upper triangular matrix, the diagonal entries are the eigenvalues. What is the meaning of the upper triangular coefficients? 

This example shows there is \emph{some} information in the upper triangular entries of an upper-triangular matrix, but the question is how much? We would like to always diagonalise $A$, but this example shows that it isn't always possible. Let's understand when it is possible. \lecturemarker{12}{31 Oct}

% \begin{proposition}
% \mbox{}
% \begin{enumerate}
% 	\item If $v_1,\ldots,v_k$ are eigenvectors with eigenvalues $\lambda_1,\ldots,\lambda_k$, and $\lambda_i\neq \lambda_j$ if $i\neq j$, then $v_1,\ldots,v_k$ are linearly independent.
% 	\item If $V_{\lambda_1},\ldots,V_{\lambda_r}$ are non-zero eigenspaces and $\lambda_i\neq \lambda_j$ for $i\neq j$, then $\sum_i V_{\lambda_i}$ is a direct sum.
% \end{enumerate}
% \end{proposition}
% 
% \begin{proof}
% 	(a) $\implies$ (b): if $0=v_1+\cdots+v_r$, with $v_i\in V_{\lambda_i}$, then we must show each $v_i=0$. But not, we get a relation a linear dependence among non-zero eigenvectors with distinct eigenvalues, contradicting (a).
% \end{proof}

\begin{proposition}
	If $v_1,\ldots,v_k$ are eigenvectors with eigenvalues $\lambda_1,\ldots,\lambda_k$, and $\lambda_i\neq \lambda_j$ if $i\neq j$, then $v_1,\ldots,v_k$ are linearly independent.
\end{proposition}

\begin{proof}
	[Proof 1] Induct on $k$. This is clearly true when $k=1$. Now if the result is false, then there are $a_i\iF$ such that $\sum_{i=1}^k a_i \, v_i=0$, with some $a_i\neq 0$, and without loss of generality, $a_1\neq 0$. (In fact, all $a_i\neq 0$, as if not, we have a relation of linearly dependence among $(k-1)$ eigenvectors, contradicting our inductive assumption.)
	
	Apply $\alpha$ to $\sum_{i=1}^k a_i \, v_i = 0$, to get
	\begin{equation*}
		\sum_{i=1}^k \lambda_i \, a_i \, v_i = 0.
	\end{equation*}
	Now multiply $\sum_{i=1}^k a_i \, v_i$ by $\lambda_1$, and we get
	\begin{equation*}
		\sum_{i=1}^k \lambda_1 \, a_i \, v_i = 0.
	\end{equation*}
	Subtract these two, and we get
	\begin{equation*}
		\sum_{i=2}^k \underbrace{\left( \lambda_i-\lambda_1 \right)}_{\neq 0} a_i \, v_i = 0,
	\end{equation*}
	a relation of linear dependence among $v_2,\ldots,v_k$, so $a_i=0$ for all $i$, by induction.
\end{proof}

\emph{Proof 2.} Suppose $\sum a_i \, v_i = 0$. Apply $\alpha$, we get $\sum \lambda_i \, a_i \, v_i = 0$; apply $\alpha^2$, we get $\sum \lambda_i^2 \, a_i \, v_i=0$, and so on, so $\sum_{i=1}^k \lambda_i^r \, a_i \, v_i = 0$ for all $r\geq 0$. In particular,
\begin{equation*}
	\mat{1 & \cdots & 1 \\ \lambda_1 & \cdots & \lambda_k \\ \vdots & & \vdots \\ \lambda_1^{k-1} & \cdots & \lambda_k^{k-1}} \mat{a_1 \,v_1 \\ \vdots \\ \vdots \\ a_k \,v_k} = 0.
\end{equation*}

\begin{lemma}
	[The Vandermonde determinant] The determinant of the above matrix is $\prod_{i<j} \left( \lambda_j - \lambda_i \right)$.
\end{lemma}
\begin{proof} Exercise!
\end{proof}
\vspace{-6pt}

By the lemma, if $\lambda_i\neq \lambda_j$, this matrix is invertible, and so $\left(a_1 \, v_1, \ldots, a_k \, v_k\right)^\Trans=0$; that is, $a_i=0$ for all $i$. \qed

Note these two proofs are the same: the first version of the proof was surreptitiously showing that the Vandermonde determinant was non-zero. It looks like the first proof is easier to understand, but the second proof makes clear what's actually going on.

	\pagebreak

\begin{definition}
	A map $\alpha$ is \emph{diagonalisable} if there is some basis for $V$ such that the matrix of $\alpha:V\to V$ is diagonal.
\end{definition}

\begin{corollary}
	The map $\alpha$ is diagonalisable if and only if $\chi_\alpha(x)$ factors into $\prod_{i=1}^r \left( x-\lambda_i \right)^{n_i}$, and $\dim V_{\lambda_i} = n_i$ for all $i$.
\end{corollary}

\begin{proof}
	($\Rightarrow$) $\alpha$ is diagonalisable means that in some basis it is diagonal, with $n_i$ copies of $\lambda_i$ in the diagonal entries, hence the characteristic polynomial is as claimed.
	
	($\Leftarrow$) $\sum_i V_{\lambda_i}$ is a direct sum, by the proposition, so
	\begin{equation*}
		\textstyle
		\dim\left( \sum_i V_{\lambda_i} \right)
		= \sum \dim V_{\lambda_i}
		= \sum n_i,
	\end{equation*}
	and by our assumption, $n=\dim V$. Now in any basis which is the union of basis for the $V_{\lambda_i}$, the matrix of $\alpha$ is diagonal.
\end{proof}

\begin{corollary}
	If $A$ is conjugate to an upper triangular matrix with $\lambda_i$ as the diagonal entries, and the $\lambda_i$ are distinct, then $A$ is conjugate to the diagonal matrix with $\lambda_i$ in the entries.
\end{corollary}

\begin{example}
	$\mat{1 & 7 \\ 0 & 2}$ is conjugate to $\mat{1 & 0 \\ 0 & 2}$.
	
	The upper triangular entries ``contain no information''. That is, they are an artefact of the choice of basis.
\end{example}

\begin{remark}
	If $\F=\C$, then the diagonalisable $A$ are dense in $\Mat_n(\C)=\C^{n^2}$ (exercise). In general, if $\F=\overline{\F}$, then diagonalisable $A$ are dense in $\Mat_n(\F)=\F^{n^2}$, in the sense of algebraic geometry.
\end{remark}

\begin{exercise}
	If $A=\mat{\lambda_1  &   \cdots & a_n \\ & \ddots & \vdots \\ 0 & & \lambda_n }$, then $Ae_i=\lambda e_i+\sum_{j<i} a_{ji} \, e_j$.
	
	Show that if $\lambda_1,\ldots,\lambda_n$ are distinct, then you can ``correct'' each $e_i$ to an eigenvector $v_i$ just by adding smaller terms; that is, there are $p_{ji}\iF$ such that
	\begin{equation*}
		v_i = e_i + \sum_{j<i} p_{ji} e_j \text{ has } Av_i = \lambda_i v_i,
	\end{equation*}
	which gives yet another proof of our proposition.
\end{exercise}

%We now need a way to disentangle different eigenvalues.
% subsection eigenvectors_and_eigenvalues (end)

	\pagebreak

\subsection{Cayley-Hamilton theorem}  % (fold)
\label{sub:cayley_hamilton_theorem}

Let $\alpha:V\to V$ be a linear map, and $V$ a finite dimensional vector space over $\F$.

\begin{theorem}
	[Cayley-Hamilton theorem] Every square matrix over a commutative ring (such as $\R$ or $\C$) satisfies $\chi_A(A)=0$.
\end{theorem}

\begin{example}
	$A=\mat{2 & 3 \\ 1 & 2}$ and $\chi_A(x)=x^2-4x+1$, so $\chi_A(A) = A^2-4A+I$. Then
	\begin{equation*}
		A^2 = \mat{7 & 12 \\ 4 & 7},
	\end{equation*}
	which does equal $4A-I$.
\end{example}

\begin{remark}
	We have
	\begin{equation*}
		\chi_A(x) = \det(xI-A) = \det \mat{x-a_{11} & \cdots & -a_{1n} \\ \vdots & \ddots & \vdots \\ -a_{n1} & \cdots & x-a_{nn}} = x^n - e_1 x^{n-1} + \cdots \pm e_n,
	\end{equation*}
	so we don't get a proof by saying $\chi_\alpha(\alpha)=\det(\alpha-\alpha)=0$. This just doesn't make sense. However, you can make it make sense, and our second proof will do this. 
\end{remark}

\begin{proof}
	[Proof 1] If $A$ is upper triangular and can be written as
	\begin{equation*}
		A=\mat{\lambda_1 & & * \\ & \ddots \\ 0 & & \lambda_n}
	\end{equation*}
	then the characteristic polynomial is
	\begin{equation*}
		\chi_A(x) = \disp\prod_{i=1}^n \left( x-\lambda_i \right).
	\end{equation*}
	Now if $A$ were in fact diagonal, then
	\begin{equation*}
		\chi_A(A) =
		\disp\prod_{i=1}^n \left( A-\lambda_i I \right) = 
		\mat{0 & & & 0 \\ & * \\ & & * \\ 0 & & & *}
		\mat{* & & & 0 \\ & 0 \\ & & * \\ 0 & & & *}
		\cdots
		\mat{* & & & 0 \\ & * \\ & & * \\ 0 & & & 0}
               = 0
	\end{equation*}
	But even when $A$ is upper triangular, this is zero. 	
	\begin{example} The matrix
		\begin{equation*}
			\mat{0 & * & * \\ & * & * \\ & & *}
			\mat{* & * & * \\ & 0 & * \\ & & *}
			\mat{* & * & * \\ & * & * \\ & & 0}
		\end{equation*}
		is still zero.
	\end{example}

		\pagebreak

	Here is a nicer way of writing this:

	Let $W_0=\{0\}$, $W_i = \left\langle e_1,\ldots,e_i \right\rangle\leq V$. If $A$ is upper triangular, then $AW_i \subseteq W_i$, and even $\left( A-\lambda_i I \right)W_i \subseteq W_{i-1}$. So $\left( A-\lambda_n I \right)W_n \subseteq W_{n-1}$, and so
	\begin{equation*}
		\left( A-\lambda_{n-1}I \right) \left( A-\lambda_n I \right) W_n \subseteq \left( A-\lambda_{n-1} I \right)W_{n-1} \subseteq W_{n-2},
	\end{equation*}
	and so on, until
	\begin{equation*}
		\prod_{i=1}^n \left( A-\lambda_iI \right) W_n \subseteq W_0 = \{0\};
	\end{equation*}
	that is, $\chi_A(A)=0$.

	If $\F=\overline{\F}$, then we can choose a basis for $V$ such that $\alpha:V\to V$ has an upper-triangular matrix with respect to this basis, and hence the above shows $\chi_A(A)=0$; that is, $\chi_A(A)=0$ for all $A\in\Mat_n(\overline{\F})$.
	
	If $\F\subseteq\overline{\F}$, then as Cayley-Hamilton is true for all $A\in\Mat_n(\overline{\F})$, then it is certainly still true for $A\in\Mat_n(\F) \subseteq \Mat_n(\overline{\F})$.
\end{proof}

\begin{definition}
	The \emph{generalised eigenspace with eigenvalue $\lambda$} is given by
	\begin{equation*}
		  V^\lambda
		= \left\{v\in V : \left( \alpha-\lambda I \right)^{\dim V}(v)=0\right\}
		= \ker\left( \lambda I-\alpha \right)^{\dim V}
		: V \to V.
	\end{equation*}
	Note that $V_\lambda \subseteq V^\lambda$.
\end{definition}

\begin{example}
	Let $A=\mat{\lambda & & * \\ & \ddots \\ 0 & & \lambda}$.
	
	Then $\left( \lambda I-A \right)e_i$ has stuff involving $e_1,\ldots,e_{i-1}$, so $\left( \lambda I-A \right)^{\dim V} e_i=0$ for all $i$, as in our proof of Cayley-Hamilton (or indeed, by Cayley-Hamilton).
	
	Further, if $\mu\neq \lambda$, then
	\begin{equation*}
		\mu I-A = \mat{\mu-\lambda & & * \\ & \ddots \\ 0 & & \mu-\lambda}
	\end{equation*}
	and so
	\begin{equation*}
		\left( \mu I-A \right)^n = \mat{\left( \mu-\lambda \right)^n & & * \\ & \ddots \\ 0 & & \left( \mu-\lambda \right)^n}
	\end{equation*}
	has non-zero diagonal terms, and so trivial kernel. Thus in this case, $V^\lambda=V$, $V^\mu = 0$ if $\mu\neq \lambda$, and in general $V^\mu=0$ if $\chi_\alpha(\mu)\neq 0$, that is, $\ker\left( A-\mu I \right)^N = \{0\}$ for all $N\geq 0$.
\end{example}

	\pagebreak

\begin{theorem}
	If $\chi_A(x)=\prod_{i=1}^r \left( x-\lambda_i \right)^{n_i}$, with the $\lambda_i$ distinct, then \lecturemarker{13}{2 Nov}
	\begin{equation*}
		V \cong \bigoplus_{i=1}^r  V^{\lambda_i},
	\end{equation*}
	and $\dim V^{\lambda_i}=n_i$. In other word, choose any basis of $V$ which is the union of the bases of the $V^{\lambda_i}$. Then the matrix of $\alpha$ is block diagonal. Moreover, we can choose the basis of each $V^\lambda$ so that each diagonal block is upper triangular, with only one eigenvalue--- $\lambda$--on its diagonals.
	
	We say ``different eigenvalues don't interact''.
\end{theorem}
\vspace{3pt}
\begin{remark}
	If $n_1=n_2=\cdots=n_r=1$ (and so $r=n$), then this is our previous theorem that matrices with distinct eigenvalues are diagonalisable.
\end{remark}

\begin{proof}
	Consider
	\begin{equation*}
		h_{\lambda_i}(x)
		= \prod_{j\neq i} \left( x-\lambda_j \right)^{n_j}
		= \f{\chi_\alpha(x)}{\left( x-\lambda_i \right)^{n_i}}.
	\end{equation*}
	Then define
	\begin{equation*}
		W^{\lambda_i} = \Im\left( h_{\lambda_i}(A):V\to V \right) \leq V.
	\end{equation*}
	Now Cayley-Hamilton implies that
	\begin{equation*}
		\underbrace{\left( A-\lambda_iI \right)^{n_i} h_{\lambda_i}(A)}_{=\chi_A(A)} = 0,
	\end{equation*}
	that is,
	\begin{equation*}
		W^{\lambda_i}
		\subseteq \ker\left( A-\lambda_iI \right)^{n_i}
		\subseteq \ker\left( A-\lambda_iI \right)^n
		= V^{\lambda_i}.
	\end{equation*}
	We want to show that
	\begin{enumerate}
		\shortskip
		\item $\sum_i W^{\lambda_i}=V$;
		\item This sum is direct.
	\end{enumerate}
	Now, the $h_{\lambda_i}$ are coprime polynomials, so Euclid's algorithm implies that there are polynomials $f_i\in F[x]$ such that
	\begin{equation*}
		\sum_{i=1}^r f_i\,h_{\lambda_i}=1,
	\end{equation*}
	and so
	\begin{equation*}
		\sum_{i=1}^r h_{\lambda_i}(A)\,f_i(A) = I \subset \End(V).
	\end{equation*}
	Now, if $v\in V$, then this gives
	\begin{equation*}
		v=\sum_{i=1}^r \underbrace{h_{\lambda_i}(A) \,f_i(A)\,v}_{\in W^{\lambda_i}},
	\end{equation*}
	that is, $\sum_{i=1}^r W^{\lambda_i}=V$. This is (i).
	
	To see the sum is direct: if $0=\sum_{i=1}^r w_i$, $w_i\in W^{\lambda_i}$, then we want to show that each $w_i=0$. But $h_{\lambda_j}(w_i) = 0$, $i\neq j$ as $w_i\in\ker\left( A-\lambda_iI \right)^{n_i}$, so (i) gives
	\begin{equation*}
		w_i =
		\sum_{i=1}^r h_{\lambda_i}(A)\,f_j(A)\,w_i
		= f_i(A)\,h_{\lambda_i}(A)\,(w_i),
	\end{equation*}
	so apply $f_i(A)\,h_{\lambda_i}(A)$ to $\sum_{i=1}^r w_i=0$ and get $w_i=0$.
\end{proof}

Now define
\begin{equation*}
	\pi_i = f_i(A)\,h_{\lambda_i}(A) = h_{\lambda_i}(A)\,f_i(A).
\end{equation*}
We showed that $\pi_i:V\to V$ has
\begin{equation*}
	\Im \pi_i = W^{\lambda_i} \subseteq V^{\lambda_i}
	\text{ and }
	\pi_i \vert W^{\lambda_i} = \text{identity, and so } \pi_i^2 = \pi_i,
\end{equation*}
that is, $\pi_i$ is the projection to $W^{\lambda_i}$. Compare with $h_{\lambda_i}(A)=h_i$, which has $h_i(V)=W^{\lambda_i}\subseteq V^{\lambda_i}$, $h_i \mid V^{\lambda_i}$ an isomorphism, but not the identity; that is, $f_i \mid V^{\lambda_i} = h_i^{-1} \mid V^{\lambda_i}$.

This tells us to understand what matrices look like up to conjugacy, it is enough to understand matrices with a single eigenvalue $\lambda$, and by subtracting $\lambda I$ from our matrix we may as well assume that eigenvalue is zero.

Before we continue investigating this, we digress and give another proof of Cayley-Hamilton.

\begin{proof}
	[Proof 2 of Cayley-Hamilton] Let $\varphi:V\to V$ be linear, $V$  finite dimensional over $\F$. Pick a basis $e_1,\ldots,e_n$ of $V$, so $\varphi(e_i) = \sum_j a_{ji}\,e_j$, and we have the matrix $A=(a_{ij})$. Consider
	\begin{equation*}
		\varphi I - A^\Trans =
		\mat{\varphi-a_{11} & \cdots & -a_{n1} \\ \vdots & \ddots & \vdots \\ -a_{1n} & \cdots & \varphi-a_{nn}} \in {\Mat}_n(\End(V)),
	\end{equation*}
	where $a_{ij}\iF \injto \End(V)$ by regarding an element $\lambda$ as the operation of scalar multiplication $V \to V$, $v \mapsto \lambda v$.
% on the $n\times n$ matrix of linear operations. 
	The elements of $\Mat_n(\End(V))$ act on $V^n$ by the usual formulas. So
	\begin{equation*}
		(\varphi I-A^\Trans)
		\mat{e_1 \\ \vdots \\ e_n}
		=
		\mat{0 \\ \vdots \\ 0}
	\end{equation*}
	by the definition of $A$.
	
	The problem is, it isn't clear how to define $\det:\Mat_n(\End(V)) \to \End(V)$, as the matrix coefficients (that is, elements of $\End(V)$) do not commute in general. But the matrix elements of the above matrix do commute, so this shouldn't be a problem.
	
	To make it not a problem, consider $\varphi I-A^\Trans\in\Mat_n(F[\varphi])$; that is, $F[\varphi]$ are polynomials in the symbol $\varphi$. This is a commutative ring and now $\det$ behaves as always:%
	\begin{enumerate}
		\shortskip
		\item $\det(\varphi I-A^\Trans) = \chi_A(\varphi)\in F[\varphi]$ (by definition);
		\item $\adj(\varphi I-A^\Trans) \cdot (\varphi I-A^\Trans) = \det(\varphi I - A^\Trans) \cdot I \in \Mat_n(F[\varphi])$, as we've shown.
	\end{enumerate}
	This is true for any $B\in\Mat_n(R)$, where $R$ is a commutative ring.  Here $R=F[\varphi]$, $B=\varphi I-A^\Trans$.
	
	Make $F[\varphi]$ act on $V$, by $\sum_i a_i\,\varphi^i:v\mapsto \sum_i a_i\,\varphi^i(v)$, so
	\begin{equation*}
		(\varphi I-A^\Trans) \mat{e_1 \\ \vdots \\ e_n} = \mat{0 \\ \vdots \\ 0}.
	\end{equation*}
	Thus
	\begin{equation*}
		0
		= \adj(\varphi I-A^\Trans) \underbrace{(\varphi I-A^\Trans) \mat{e_1 \\ \vdots \\ e_n}}_{=0}
		= \det(\varphi I-A^\Trans)\mat{e_1 \\ \vdots \\ e_n}
		= \mat{\chi_A(\varphi)\,e_1 \\ \vdots \\ \chi_A(\varphi)\,e_n}.
	\end{equation*}
	So this says that $\chi_A(A)\,e_i = \chi_A(\varphi)\,e_i=0$ for all $i$, so $\chi_A(A):V\to V$ is the zero map, as $e_1,\ldots,e_n$ is a basis of $V$; that is, $\chi_A(A)=0$.	
\end{proof}

This correct proof is as close to the nonsense tautological ``proof'' (just set $x $ equal to $A$) as you can hope for. You will meet it again several times in later life, where it is called \emph{Nakayama's lemma}.

% subsection cayley_hamilton_theorem (end)

	% \pagebreak

\subsection{Combinatorics of nilpotent matrices}  % (fold)
\label{sub:combinatorics_of_nilpotent_matrices}

\lecturemarker{14}{5 Nov}
\begin{definition}
	If $\varphi:V\to V$ can be written in block diagonal form; that is, if there are some $W\p,W\pp\leq V$ such that
	\begin{equation*}
		\varphi(W\p)\subseteq W\p
		\qquad
		\varphi(W\pp) \subset W\pp
		\qquad
		V=W\p\oplus W\pp
	\end{equation*}
	then we say that $\varphi$ is \emph{decomposable} and write
	\begin{equation*}
		\varphi=\varphi\p\oplus\varphi\pp
		\qquad
		\varphi\p = \eval[1]{\varphi}_{W\p}:W\p\to W\p
		\qquad
		\varphi\pp = \eval[1]{\varphi}_{W\pp}:W\pp\to W\pp
	\end{equation*}
	We say that $\varphi$ is the \emph{direct sum} of $\varphi\p$ and $\varphi\pp$. Otherwise, we say that $\varphi$ is \emph{decomposable}.
\end{definition}

\begin{examples}
\mbox{}
\begin{enumerate}
	\item $\varphi=\mat{0 & 0 \\ 0 & 0} = (0:\F\to\F) \oplus (0:\F\to\F)$
	\item $\varphi=\mat{0 & 1 \\ 0 & 0}:\F^2\to\F^2$ is indecomposable, as $\left\langle e_1 \right\rangle$ is a unique $\varphi$-stable line.
	\item If $\varphi:V\to V$, then $\chi_\varphi(x) = \prod_{i=1}^r \left( x-\lambda_i \right)^{n_i}$, for $\lambda_i\neq \lambda_j$ if $i\neq j$.
	
	Then $V=\bigoplus_{i=1}^r V^{\lambda_i}$ decomposes $\varphi$ into pieces $\varphi_{\lambda_i} = \eval[1]{\varphi}_{V^{\lambda_i}}:V^{\lambda_i} \to V^{\lambda_i}$ such that each $\varphi_\lambda$ has only one eigenvalue, $\lambda$. 

	\emph{This decomposition is exactly the amount of information in} $\chi_\varphi(x)$. So we need new information to further 
	understand what matrices are up to conjugacy.

	Observe that $\varphi_\lambda$ is decomposable if and only if $\varphi_\lambda-\lambda I$ is, and the only eigenvalue of $\varphi_\lambda-\lambda I$ is zero.
\end{enumerate}
\end{examples}

\begin{definition}
	The map $\varphi$ is \emph{nilpotent} if the following (equivalent) conditions apply:
	\begin{itemize}
		\shortskip
		\item $\varphi^{\dim V} = 0$;
		\item $\ker\varphi^{\dim V}=V$;
		\item $V^0=V$;
		\item $\chi_\varphi(x) = x^{\dim V}$, and the only eigenvalue is zero.
	\end{itemize}
\end{definition}

	\pagebreak

\begin{theorem}
	Let $\varphi$ be nilpotent. Then $\varphi$ is indecomposable if and only if there is a basis $v_1,\ldots,v_n$ such that
	\begin{equation*}
		\varphi(v_i) =
		\begin{cases}
			0 & \text{if } i=1, \\
			v_{i-1} & \text{if } i>1,
		\end{cases}
	\end{equation*}
	that is, if the matrix of $\varphi$ is
	\renewcommand{\arraystretch}{1.0}
	\begin{equation*}
		J_n = \mat{0 & 1 & ~ & 0 \\ & \ddots & \ddots \\ & & \ddots & 1 \\ 0 & ~ & ~ & 0}
	\end{equation*}
	\drangarray
	This is the \emph{Jordan block of size $n$ with eigenvalue 0}.
	\label{thm:jordan-1}
\end{theorem}

\begin{definition}
	The \emph{Jordan block of size $n$, eigenvalue $\lambda$}, is given by
	\begin{equation*}
		J_n(\lambda) = \lambda I + J_n.
	\end{equation*}
\end{definition}

\begin{theorem}
	[Jordan normal form] Every matrix is conjugate to a direct sum of Jordan blocks. Morever, these are unique up to rearranging their order.
	\label{thm:jordan-2}
\end{theorem}

\begin{proof}
Observe that theorem~\ref{thm:jordan-2} $\implies$ theorem~\ref{thm:jordan-1}, if we show that $J_n$ is indecomposable (and theorem \ref{thm:jordan-1} $\implies$ theorem~\ref{thm:jordan-2}, existence).
	

	[Proof of Theorem \ref{thm:jordan-1}, $\Leftarrow$] 
	Put $W_i=\left\langle v_1,\dots,v_i \right\rangle$.
% and $\varphi(v_i) = \begin{cases} 0 & i=1, \\ v_{i-1} & i>1 \\ \end{cases}$.
	
	Then $W_i$ is the \emph{only} subspace $W$ of $\dim i$ such that $\varphi(W)\subseteq W$, a $\varphi$-invariant subspace of $V$ of $\dim i$, 
	and $W_{n-i}$ is \emph{not} a complement to it, as $W_i \cap W_{n-i} = W_{\min(i,n-i)}$.
\end{proof}

\begin{proof}
	[Proof of Theorem \ref{thm:jordan-2}, uniqueness] Suppose $\alpha:V\to V$, $\alpha$ nilpotent and $\alpha=\bigoplus_{i=1}^r J_{k_i}$. Rearrange their order so that $k_i\geq k_j$ for $i\geq j$, and group them together, so $\bigoplus_{i=1}^r m_i J_i$.
	
	There are $m_i$ blocks of size $i$, and
	\begin{equation*}
		m_i = \#\left\{k_a \mid k_a = i\right\}. \tag{$*$}
	\end{equation*}
	\vspace{-9pt}
	\begin{example}
		If $(k_1,k_2,\dots) = (3,3,2,1,1,1)$, then $n=11$, and $m_1 = 3, m_2 =1, m_3 =2$, and $ m_a = 0 $ for $a > 3$. (It is customary to omit the zero entries when listing these numbers).
	\end{example}

	\begin{definition}
		Let $\cal{P}_n = \{ (k_1,k_2, \dots, k_n) \in \N^n \mid k_1 \geq k_2 \geq \dots \geq k_n \geq 0, \sum k_i = n\}$ be the set of \emph{partitions of $n$}. This is isomorphic to the set $\{ m : \N \to \N \mid \sum_i im(i) = n \}$ as above.
	\end{definition}

		\pagebreak

	We represent $\bf{k}\in\cal{P}_n$ by a picture, with a row of length $k_j$ for each $j$ (equivalently, with $m_i$ rows of length $i$). For example, the above partition $(3,3,2,1,1,1)$ has picture
	\begin{equation*}
		\bf{k} =
		\begin{array}{ccc}
			X & X & X \\
			X & X & X \\
			X & X \\
			X \\
			X \\
			X,
		\end{array}
	\end{equation*}
	Now define $\bf{k}^\Trans$, if $\bf{k}\in\cal{P}_n$, the \emph{dual partition}, to be the partition attached to the transposed diagram.

	In the above example $\bf{k}^\Trans = (6,3,2)$. 
	
	It is clear that $\bf{k}$ determines $\bf{k}^\Trans$. In formulas:
	\begin{equation*}
		\bf{k}^\Trans = (m_1+m_2+m_3+\dots+m_n, m_2 + m_3 + \dots + m_n, \dots, m_n)
%\left( \sum_{i=1}^n m_i, \sum_{i=2}^n m_i, \ldots, \sum_{i=n-1}^n m_i, m_n \right).
	\end{equation*}

	Now, let $\alpha:V\to V$, and $\alpha=\bigoplus_{i=1}^r J_{k_i} = \bigoplus_{i=1}^r m_i J_i$ as before. Observe that
	\begin{align*}
		\dim\ker\alpha
		&= \# \text{ of Jordan blocks}
		 = r
		 = \sum_{i=1}^n m_i
		 = (\bf{k}^\Trans)_1 \\
		\dim\ker\alpha^2
		&= \# \text{ of Jordan blocks of sizes 1 and 2}
		 = \sum_{i=1}^n m_i + \sum_{i=2}^n m_i
		 = (\bf{k}^\Trans)_1 + (\bf{k}^\Trans)_2 \\[-3pt]
		 &\;\,\vdots \\[-3pt]
		\dim\ker\alpha^n
		&= \# \text{ of Jordan blocks of sizes 1,}\ldots,n
		 = \sum_{k=1}^n \sum_{i=k}^n m_i
		 = \sum_{i=1}^n (\bf{k}^\Trans)_i.
	\end{align*}
	That is, $\dim \ker\alpha,\ldots,\dim\ker\alpha^n$ determine the dual partition to the partition of $n$ into Jordan blocks, and hence determine it.
	 
	It follows that the decomposition $\alpha=\bigoplus_{i=1}^n m_i J_i$ is unique.
\end{proof}
\vspace{3pt}
\begin{remark}
	This is a practical way to compute JNF of a matrix $A$. First computer $\chi_A(x)=\prod_{i=1}^r \left( x-\lambda_i \right)^{n_i}$, then compute eigenvalues with $\ker(A-\lambda_iI), \ker\left( A-\lambda_iI \right)^2,\ldots,\ker\left( A-\lambda_iI \right)^n$.
\end{remark}

\vspace{3pt}
\begin{corollary}
	The number of nilpotent conjugacy classes is equal to the size of $\cal{P}_n$.
\end{corollary}
\vspace{-6pt}
\begin{exercises}
\mbox{}
\begin{enumerate}
	\shortskip
	\item List all the partitions of $\{1,2,3,4,5\}$; show there are $7$ of them.
	\item Show that the size of $\cal{P}_n$ is the coefficient of $x^n$ in
	\begin{align*}
		\prod_{i\geq 1} \f{1}{1-x^i}
		&= (1 + x + x^2 + x^3 + \cdots)(1+x^2 + x^4 + x^6 + \cdots)(1 + x^3 + x^6 + x^9 + \cdots)\cdots \\[-9pt]
		&= \prod_{k\geq 1} \sum_{i=0}^\infty x^{ki}
	\end{align*}
\end{enumerate}
\end{exercises}

	\pagebreak

\begin{theorem}
	[Jordan Normal Form] Every matrix is conjugate to a direct sum of Jordan blocks \lecturemarker{15}{7 Nov}
	\begin{equation*}
		J_n(\lambda) = \mat{\lambda & 1 & & 0 \\
		& \ddots & \ddots \\
		& & \ddots & 1 \\
		0 & & & \lambda}
	\end{equation*}
\end{theorem}

\begin{proof}
	It is enough to show this when $\varphi:V\to V$ has a single generalised eigenspace with eigenvalue $\lambda$, and now replacing $\varphi$ by $\varphi-\lambda I$, we can assume that $\varphi$ is nilpotent.
	
	Induct on $n=\dim V$. The case $n=1$ is clear.
	
	\newcommand{\ooplus}{\underset{\underset{\underset{\bigoplus}}{}}{,}}
	
	Consider $V\p=\Im\varphi=\varphi(V)$. Then $V\p\neq V$, as $\varphi$ is nilpotent, and $\varphi(V\p)\subseteq \varphi(V) = V\p$, and $\varphi\mid V\p:V\p\to V\p$ is obviously nilpotent, so induction gives the existence of a basis
	\begin{equation*}
		\underbrace{e_1,\ldots,e_{k_1}}_{J_{k_1}}
		\oplus
		\underbrace{e_{k_1+1},\ldots,e_{k_1+k_2}}_{J_{k_2}}
		\oplus
		\ldots
		\oplus
		\underbrace{\ldots,e_{k_1+\ldots+k_r}}_{J_{k_r}}
	\end{equation*}
	such that $\varphi\mid V\p$ is in JNF with respect to this basis.
	
	Because $V\p=\Im\varphi$, it must be that the tail end of these strings is in $\Im\varphi$; that is, there exist $b_1,\ldots,b_r\in V\backslash V\p$ such that $\varphi(b_i) = e_{k_1 + \cdots+ k_i}$, as $e_{k_1 + \cdots + k_i}\not\in\varphi(V\p)$. Notice these are linearly independent, as if $\sum \lambda_i \, b_i=0$, then
	\begin{equation*}
		\sum \lambda_i \, \varphi(b_i) = \sum \lambda_i \, e_{k_1+\cdots+k_i} = 0.
	\end{equation*}
	But $e_{k_1},\ldots,e_{k_1+\ldots+k_r}$ are linearly independent, hence $\lambda_1=\cdots=\lambda_r=0$. Even better: $\{e_j,b_i \mid j\leq k_1+\ldots+k_r, 1\leq i\leq r\}$ are linearly independent. (Proof: exercise.)
	
	Finally, extend $\underbrace{e_1,e_{k_1+1},\ldots,e_{k_1+\ldots+k_{r-1}+1}}_\text{basis of $\ker\varphi\cap\Im\varphi$}$ to a basis of $\ker\varphi$, by adding basis vectors.
	
	Denote these by $q_1,\ldots,q_s$. \textbf{Exercise.} Show $\{e_j,b_i,q_k\}$ are linearly independent. \hfill $(**)$
	
	Now, the rank-nullity theorem shows that $\dim \Im \varphi + \dim \ker \varphi = \dim V$. But $\dim \Im\varphi $ is the number of the $e_i$, that is $k_1 + \dots + k_r$, and $\dim \ker \varphi$ is the number of Jordan blocks, which is $r + s$, ($r$ is the number of blocks of size greater than one, $s$ the number of size one), which is the number of $b_i$ plus the number of $q_k$.

%	\begin{equation*}
%		\underset{\substack{\text{number of } e_i\text{'s} \\ = k_1 + \ldots + k_r}}{\dim \Im \varphi}
%		+
%		\underset{\substack{\text{number of blocks of size bigger than 1 } = r \\ + \text{number of blocks of size 1 } = q \\ \text{number of } b_i\text{'s} \\ + \text{number of } q_k\text{'s}}}{\dim \ker \varphi} = \dim V.
%	\end{equation*}

	So this shows that $e_j,b_i,q_k$ are a basis of $V$, and hence with respect to this basis,
	\begin{equation*}
		\varphi = J_{k_1+1} \oplus \cdots \oplus J_{k_r+1} \oplus \underbrace{J_1 \oplus \cdots \oplus J_1}_{s \text{ times}} \qedhere
	\end{equation*}
\end{proof}

% subsection combinatorics_of_nilpotent_matrices (end)

	\pagebreak

\subsection{Applications of JNF}  % (fold)
\label{sub:applications_of_jnf}

\begin{definition}
	Suppose $\alpha:V\to V$. The \emph{minimum polynomial of $\alpha$} is a monic polynomial $p(x)$ of smallest degree such that $p(\alpha)=0$.
\end{definition}

\begin{lemma}
	If $q(x)\in F[x]$ and $q(\alpha)=0$, then $p\divides q$.
\end{lemma}

\begin{proof}
	Write $q=pa+r$, with $a,r\in F[x]$ and $\deg r < \deg p$. Then
	\begin{equation*}
		0 = q(\alpha) = p(\alpha)\,a(\alpha) + r(\alpha) = r(\alpha)
		\implies r(\alpha)=0,
	\end{equation*}
	which contradicts $\deg p$ as minimal unless $r=0$.
\end{proof}

As $\chi_\alpha(\alpha)=0$, $p(x) \divides \chi_\alpha(x)$, and in particular, it exists. (And by our lemma, is unique.) Here is a cheap proof that  the minimum polynomial
exists, which doesn't use Cayley Hamilton.

\begin{proof}
	$I, \alpha, \alpha^2,\dots,\alpha^{n^2}$ are $n^2 +1$ linear functions in $\End V$, a vector space of dimension $n^2$. Hence there must be a relation of linear dependence, $\sum_0^{n^2} a_i \, \alpha^i = 0$, so $q(x) = \sum_0^{n^2} a_i \, x^i$ is a polynomial with $q(\alpha) = 0$.
\end{proof}

Now, lets use JNF to determine the minimial polynomial.
\begin{exercise} Let $A \in Mat_n(\F)$, $\chi_A(x) = (x-\lambda_1)^{n_1}\cdots(x-\lambda_r)^{n_r}$. Suppose that the maximal size of a Jordan block with eigenvalue $\lambda_i$ is $k_i$. (So $k_i \leq n_i$ for all $i$). 
Show that the minimum polynomial of $A$ is $(x-\lambda_1)^{k_1}\cdots(x-\lambda_r)^{k_r}$.
\end{exercise}
So the minimum polynomial forgets most of the structure of the Jordan normal form.



\bigskip
\bigskip
Another application of JNF is we can use it to compute powers  $B^n$ of a matrix $B$ for any $n\geq 0$. First observe that $(PAP^{-1})^n = PA^nP^{-1}$  Now write $B = PAP^{-1}$ with $A$ in Jordan normal form. So to finish we must compute what the powers of elements in JNF look like. But 
\begin{equation*}
	J_n =
	\mat{
		 0 & 1 & & & & 0 \\
		 & 0 & 1 \\
		 & & \ddots \\
		 & & & \ddots \\
		 & & & & 0 & 1 \\
		 0 & & & & & 0
	},
	\qquad
	J_n^2 =
	\mat{
		0 & 0 & 1 & & & 0 \\
		& & 0 & 1 \\
		& & & \ddots \\
		& & & & 0 & 1 \\
		& & & & & 0 \\
		0 & & & & & 0
	}, \quad \dots,
	\qquad
	J_n^{n-1} =
	\mat{0 & & & & 0 & 1 \\ & & & & & 0 \\ \\ \\ \\ 0 & & & & & 0}
\end{equation*}
and 
\begin{equation*}
	\left( \lambda I+J_n \right)^a = \sum_{k\geq 0} {a \choose k} \lambda^{a-k} J_n^k.
\end{equation*}
Now assume $\F=\C$.

\begin{definition}
	$\exp A=\disp\sum_{n\geq 0} \f{A^n}{n!}$, $A\in\Mat_n(\C)$. 
\end{definition}

This is an infinite sum, and we must show it converges. This means that each matrix coefficient converges. This is very easy, but we omit here for lack of time.

\begin{example}
	For a diagonal matrix:
	\begin{equation*}
		\exp\mat{\lambda_1 & & 0 \\ & \ddots \\ 0 & & \lambda_n} = \mat{e^{\lambda_1} & & 0 \\ & \ddots \\ 0 & & e^{\lambda_n}}
	\end{equation*}
	and convergence is usual convergence of $\exp$.
\end{example}

\begin{exercises}
\mbox{}
\begin{enumerate}
	\shortskip
	\item If $AB=BA$, then $\exp(A+B) = \exp A \exp B$.
	\item Hence $\exp(J_n+\lambda I) = e^\lambda \exp(J_n)$
	\item $P\cdot \exp(A) \cdot P^{-1} = \exp(PAP^{-1})$
\end{enumerate}
\end{exercises}

So now you know how to compute $\exp(A)$, for $A\in\Mat_n(\C)$.

We can use this to solve linear ODEs with constant coefficients:

Consider the linear ODE
\begin{equation*}
	\od{\bf{y}}{t} = A\bf{y},
\end{equation*}
for $A\in\Mat_n(\C)$, $\bf{y}=(y_1(t), y_2(t), \dots, y_n(t))^T$, $y_i(t)\in C^\infty(\C)$.

\begin{example}
	Consider
	\begin{equation*}
		\od[n]{z}{t} + c_{n-1} \od[n-1]{z}{t} + \cdots + c_0 z = 0,  \tag{$**$}
	\end{equation*}
This is a particular case of the above, where $A$ is the matrix 
$$\mat{0 & 1 &    &   &   \\
        & 0 & 1  &   &    \\
        \dots             \\
        &   &    & 0 & 1  \\
      -c_0 & -c_1 & \dots & & -c_{n-1}
}.$$
To see this, consider what 
	$A\bf{y} = \bf{y}\p$ means.  Set $z=y_1$, then $y_2=y_1\p=z\p$, $y_3=y_2\p=z\pp$, \ldots, $y_n=y_{n-1}\p = \od[n-1]{z}{t}$ and $(**)$ is the last equation.
\end{example}
	
	There is a unique solution of $\od{\bf{y}}{t} = A\bf{y}$ with fixed initial conditions $y(0)$, by a theorem of analysis. On the other hand:


\begin{exercise}
	$\exp(At)\,y(0)$ is a solution, that is
	\begin{equation*}
		\od{}{t}\left( \exp(At)\,y(0) \right)=A\exp(At)\,y(0)
	\end{equation*}
	Hence it is the unique solution with value $y(0)$.
	
	Compute this when $A=\lambda I + J_n$ is a Jordan block of size $n$.
\end{exercise}

% subsection applications_of_jnf (end)
