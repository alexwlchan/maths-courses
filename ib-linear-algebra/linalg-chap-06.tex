%!TEX root = linear-algebra.tex
\stepcounter{lecture}
\setcounter{lecture}{6}
\sektion{Hermitian forms}

\renewcommand{\Im}{\mathfrak{I}}

\lecturemarker{20}{19 Nov}
A non-degenerate quadratic form on a vector space over $\C$ doesn't behave like an inner product on $\R^2$. For example,
\begin{equation*}
	\text{if } Q {x_1 \choose x_2} = x_1^2 + x_2^2
	\qquad
	\text{ then we have }
	\qquad
	Q {1 \choose i} = 1+i^2 = 0.
\end{equation*}
We don't have a notion of positive definite, but there is a modification of a notion of a bilinear form which does.

\begin{definition}
	Let $V$ be a vector space over $\C$; then a function $\psi:V\times V\to\C$ is called \emph{sesquilinear} if %
	\begin{enumerate}
		\shortskip
		\item For all $v\in V$, $\psi(\cdot,v)$, $u\mapsto \psi(u,v)$ is linear; that is%
		\begin{equation*}
			\psi(\lambda_1\,u_1 + \lambda_2\,u_2 , v) =
			\lambda_1 \psi(u_1,v) + \lambda_2\psi(u_2,v) %
		\end{equation*}
		\item For all $u,v_1,v_2\in V$, $\lambda_1,\lambda_2\iC$,
		\begin{equation*}
			\psi(u,\lambda_1\,v_1+\lambda_2\,v_2) = \overline{\lambda}_1\,\psi(u,v_1) + \overline{\lambda}_2\,\psi(u,v_2), %
		\end{equation*}
		where $\overline{z}$ is the complex conjugate of $z$. %
	\end{enumerate}
	It is called \emph{Hermitian} if it also satisfies
	\begin{enumerate}
		\setcounter{enumi}{2}
		\item $\psi(v,w) = \overline{\psi(w,v)}$ for all $v,w\in V$.
	\end{enumerate}
	Note that (i) and (iii) imply (ii).
\end{definition}

Let $V$ be a vector space over $\C$, and $\psi:V\times V\to\C$ a Hermitian form. Define
\begin{equation*}
	Q(v) = \psi(v,v) = \overline{\psi(v,v)}
\end{equation*}
by (iii), so $Q:V\to\R$.

\begin{lemma}
	We have $Q(v)=0$ for all $v\in V$ if and only if $\psi(v,w)=0$ for all $v,w\in V$. %
\end{lemma}

\begin{proof}
	We have.
	\begin{align*}
		Q(u\pm v) = \psi(u\pm v,u\pm v) %
		&= \psi(u,u) + \psi(v,v) \pm \psi(u,v) \pm \psi(v,u) \\
		&= Q(u) + Q(v) \pm 2\,\Re\,\psi(u,v),
	\end{align*}
	as $z+\overline{z}=2\,\Re(z)$. Thus
	\begin{align*}
		Q(u+v) - Q(u-v) &= 4\,\Re\,\psi(u,v), \\
		Q(u+iv) - Q(u-iv) &= 4\,\Im\,\psi(u,v),
	\end{align*}
	that is, $Q:V\to\R$ determines $\psi:V\times V\to\C$ if $Q$ is Hermitian:
	\begin{equation*}
		\psi(u,v) = \tf{1}{4}\left[ Q(u+v) + i\,Q(u+iv) - Q(u-v) - i\,Q(u-iv) \right]. \qedhere %
	\end{equation*}
\end{proof}

Note that
\begin{equation*}
	Q(\lambda v)
	= \psi(\lambda v,\lambda v)
	= \lambda\overline{\lambda}\,\psi(v,v)
	= \left\vert \lambda \right\vert^2 Q(v). %
\end{equation*}
If $\psi:V\times V\to\C$ is Hermitian, and $v_1,\ldots,v_n$ is a basis of $V$, then we write $A=(a_{ij})$, $a_{ij}=\psi(v_i,v_j)$, and we call this the \emph{matrix of $\psi$ with respect to $v_1,\ldots,v_n$}.

Observe that $A^\Trans=\overline{A}$; that is, $A$ is a Hermitian matrix.

\begin{exercise}
	Show that if we change basis, $v_j\p = \sum_i p_{ij} \, v_i$, $P=(p_{ij})$, then $A\mapsto P^\Trans A\overline{P}$. %
\end{exercise}

\begin{theorem}
	If $V$ is a finite dimensional vector space over $\C$, and $\psi:V\times V\to\C$ is Hermitian, then there is a basis for $V$ such that the matrix $A$ of $\psi$ is %
	\begin{equation*}
		\mat{I_p \\ & -I_q \\ & & 0}
	\end{equation*}
	for some $p,q\geq 0$. Moreover, $p$ and $q$ are uniquely determined by $\psi$: $p$ is the maximum dimension of a positive definite subspace $P$, and $q$ is the maximal dimension  of a negative definite subspace. %
\end{theorem}

Here $P\leq V$ is positive definite if $Q(v)\geq 0$ for all $v\in P$, and $Q(v) = 0$ when $v \in P$ implies $v = 0$; $P$	is negative definite $-Q$ is positive definite on $P$.
%if $Q(v)\leq 0$ for all $v\in P$

\vspace{3pt}

\begin{proof*}
	Exactly as for real symmetric forms, using the Hermitian ingredients before the theorem instead of their bilinear counterparts. %
\end{proof*}

\begin{definition}
	If $W\leq V$, then the \emph{orthogonal complement to $W$} is given by %
	\begin{equation*}
		W^\perp = \left\{v\in V \mid \psi(W,v) = \psi(v,W) = 0 \right\} = {}^\perp W.
	\end{equation*}
	We say that $\psi$ is \emph{non-degenerate} if $V^\perp=0$, equivalenyly  if  $p+q=\dim V$. We also define the \emph{unitary group} %
	\begin{align*}
		U(p,q)
		&= \Isom\mat{I_p \\ & -I_q} \\[3pt]
		&= \left\{X\in GL_n(\C) \mid X^\Trans \mat{I_p \\ & -I_q} \overline{X} = \mat{I_p & 0 \\ 0 & -I_q}\right\} \\[3pt] %
		&= \left\{\text{stabilizers of the form } \mat{I_p \\ & -I_q} \text{ with respect to } \textstyle\GL_n(\C) \text{ action}\right\}, %
	\end{align*}
	where the action takes $\psi \mapsto g\psi$, with $(g\psi)(x,y) = \psi(g^{-1}x, g^{-1}y)$. Again, note $g^{-1}$ here so that $(gh)\psi = g(h\psi)$. %
	
	In the special case where the form $\psi$ is positive definite, that is, conjugate to $I_n$, we call this the \emph{unitary group} %
	\begin{equation*}
		U(n) = U(n,0) = \left\{X\in\textstyle\GL_n(\C) \mid X^\Trans \overline{X} = I \right\}. %
	\end{equation*}
\end{definition}

\begin{proposition}
	Let $V$ be a vector space over $\C$ (or $\R$), and $\psi:V\times V\to\C$ (or $\R$) a Hermitian (respectively, symmetric) form, so $Q:V\to\R$. %
	
	Let $v_1,\ldots,v_n$ be a basis of $V$, and $A\in\Mat_n(\C)$ the matrix of $\psi$, so $A^\Trans=\overline{A}$. Then $Q:V\to\R$ is positive definite if and only if, for all $k$, $1\leq k\leq n$, the top left $k\times k$ submatrix of $A$ (called $A_k$) has $\det A_k\iR$ and $\det A_k>0$. %
\end{proposition}

	\pagebreak

\begin{proof}
	($\Rightarrow$) If $Q$ is positive definite, then $A=P^\Trans I\overline{P} = P^\Trans \overline{P}$ for some $P\in\GL_n(\C)$, and so %
	\begin{equation*}
		\det A = \det P^\Trans \det \overline{P} = \left\vert \det P \right\vert^2 > 0. \tag{$*$} %
	\end{equation*}
	But if $U\leq V$, as $Q$ is positive definite on $V$, it is positive definite on $U$. Take $U=\left\langle v_1,\ldots,v_k \right\rangle$, then $Q\mid U$ is positive definite, $A_k$ is the matrix of $Q \mid U$, and by $(*)$, $\det A_k > 0$. %
	
	($\Leftarrow$) Induct on $n=\dim V$. The case $n=1$ is clear. Now the induction hypothesis tells us that $\psi \mid \left\langle v_1,\ldots,v_{n-1} \right\rangle$ is positive definite, and hence the dimension of a maximum positive definite subspace is $p\geq n-1$. %
	
	So by classification of Hermitian forms, there is some $P\in\GL_n(\C)$ such that
	\begin{equation*}
		A = P^\Trans \mat{I_{n-1} & 0 \\ 0 & c} \overline{P},
	\end{equation*}
	where $c=0,1$ or $-1$. But $\det A = \left\vert \det P \right\vert^2 c > 0$ by assumption, so $c=1$, and $A=P^\Trans \overline{P}$; that is, $Q$ is positive definite. %
\end{proof}

\begin{definition}
	If $V$ is a vector space over $\F = \R$ or $\F =\C$, then an \emph{inner product on $V$} is a positive definite symmetric bilinear/Hermitian form $\left\langle \cdot,\cdot \right\rangle:V\times V \to\F$, and we say that $V$ is an \emph{inner product space}. % 
\end{definition}

\begin{example}
	Consider $\Rn$ or $\Cn$, and the dot product $\left\langle \bf{x},\bf{y} \right\rangle = \sum x_i\,\overline{y}_i$. These forms behave exactly as our intuition tells us in $\R^2$. %
\end{example}

\subsection{Inner product spaces} % (fold)
\label{sub:inner_product_spaces}

\begin{definition}
	Let $V$ be an inner product space over $\F$ with $\left\langle \cdot,\cdot \right\rangle: V\times V\to\C$. Then $Q(v) \in \R_{\geq0}$, and so we can define %
	\begin{equation*}
		\lv v \rv = {\sqrt{Q(v)}}
	\end{equation*}
	to be the \emph{length} or \emph{norm} of $v$. Note that $\left\vert v \right\vert=0$ if and only if $v=0$. %
\end{definition}

\begin{lemma}
	[Cauchy-Schwarz inequality] $\left\vert \left\langle v,w \right\rangle \right\vert \leq \lv v \rv \lv w \rv$. %
\end{lemma}

\begin{proof}
	As you've seen many times before: %
	\begin{align*}
		0
		&\leq \left\langle -\lambda v+w, -\lambda v+w \right\rangle \\
		&= \left\vert \lambda \right\vert^2 \left\langle v,v \right\rangle + \left\langle w,w \right\rangle - \lambda \left\langle v,w \right\rangle - \overline{\lambda \left\langle v,w \right\rangle}. %
	\end{align*}
	The result is clear if $v=0$, otherwise suppose $\left\vert v \right\vert\neq 0$, and put $\lambda=\overline{\left\langle v,w \right\rangle}/\left\langle v,v \right\rangle$. We get %
	\begin{equation*}
		0\leq \f{\left\vert \left\langle v,w \right\rangle \right\vert^2}{\left\langle v,v \right\rangle} - \f{2\left\vert \left\langle v,w \right\rangle \right\vert^2}{\left\langle v,v \right\rangle} + \left\langle w,w \right\rangle, %
	\end{equation*}
	that is, $\left\vert \left\langle v,w \right\rangle \right\vert^2 \leq  \left\langle v,v \right\rangle \left\langle w,w \right\rangle$. %
\end{proof}

Note that if $\F=\R$, then $\left\langle v,w \right\rangle/\left\vert w \right\vert\left\vert v \right\vert\in[-1,1]$ so there is some $\theta\in[0,\pi)$ such that $\cos\theta=\left\langle v,w \right\rangle/\left\vert v \right\vert\left\vert w \right\vert$. We call $\theta$ the \emph{angle between $v$ and $w$}.

	\pagebreak

\begin{corollary}
	[Triangle inequality] For all $v,w\in V$, $\left\vert v+w \right\vert\leq \left\vert v \right\vert+\left\vert w \right\vert$. %
\end{corollary}

\begin{proof}
	As you've seen many times before:
	\begin{align*}
		\left\vert v+w \right\vert^2
		&= \left\langle v+w,v+w \right\rangle \\
		&= \left\vert v \right\vert^2+2\Re\left\langle v,w \right\rangle+\left\vert w \right\vert^2 \\
		&\leq \left\vert v^2 \right\vert+2\left\vert v \right\vert\left\vert w \right\vert + \left\vert w^2 \right\vert \text{ (by lemma)} \\ %
		&=\left( \left\vert v \right\vert+\left\vert w \right\vert \right)^2. \qedhere
	\end{align*}
\end{proof}

\lecturemarker{21}{21 Nov} Given $v_1,\ldots,v_n$ with $\left\langle v_i,v_j \right\rangle=0$ if $i\neq j$, we say that $v_1,\ldots,v_n$ are \emph{orthogonal}. If $\left\langle v_i,v_j \right\rangle=\delta_{ij}$, then we say that $v_1,\ldots,v_n$ are \emph{orthonormal}.

So $v_1,\ldots,v_n$ orthogonal and $v_i\neq 0$ for all $i$ implies that $\hat{v}_1,\ldots,\hat{v}_n$ are orthonormal, where $\hat{v}_i = v_i/\left\vert v_i \right\vert$.

\begin{lemma}
	If $v_1,\ldots,v_n$ are non-zero and orthogonal, and if $v=\sum_{i=1}^n \lambda_i\,v_i$, then $\lambda_i = \left\langle v,v_i \right\rangle/\left\vert v_i \right\vert^2$.  %
\end{lemma}

\begin{proof}
	$\left\langle v,v_k \right\rangle = \sum_{i=1}^n \lambda_i \left\langle v_i,v_k \right\rangle = \lambda_k \left\langle v_k,v_k \right\rangle$, hence the result. %
\end{proof}

In particular, distinct orthonormal vectors $v_1,\ldots,v_n$ are linearly independent, since $\sum_i \lambda_i\,v_i = 0$ implies $\lambda_i=0$.

As $\left\langle \cdot,\cdot \right\rangle$ is Hermitian, we know there is a basis $v_1,\ldots,v_n$ such that the matrix of $\left\langle \cdot,\cdot \right\rangle$ is
\begin{equation*}
	\mat{I_p \\ & -I_q \\ & & 0}.
\end{equation*}
As $\left\langle \cdot,\cdot \right\rangle$ is positive definite, we know that $p=n$, $q=0$, $\rk=\dim V$; that is, this matrix is $I_n$. So we know there exists an orthonormal basis $v_1,\ldots,v_n$; that is $V\cong \Rn$, with $\left\langle x,y \right\rangle = \sum_i x_i\,y_i$, or $V\cong\Cn$, with $\left\langle x,y \right\rangle = \sum_i x_i\,\overline{y{_i}}$.

Here is another constructive proof that orthonormal bases exist.

\begin{theorem}
	[Gram-Schmidt orthogonalisation] Let $V$ have a basis $v_1,\ldots,v_n$. Then there exists an orthonormal basis $e_1,\ldots,e_n$ such that $\left\langle v_1,\ldots,v_k \right\rangle = \left\langle e_1,\ldots,e_k \right\rangle$ for all $1\leq k\leq n$. %
\end{theorem}

\begin{proof}
	Induct on $k$. For $k=1$, set $e_1=v_1/\left\vert v_1 \right\vert$. %
	
	Suppose we've found $e_1,\ldots,e_k$ such that $\left\langle e_1,\ldots,e_k \right\rangle = \left\langle v_1,\ldots,v_k \right\rangle$. Define %
	\begin{equation*}
		\tilde{e}_{k+1} = v_{k+1} - \sum_{1\leq i\leq k} \left\langle v_{k+1},e_i \right\rangle e_i. %
	\end{equation*}
	Thus
	\begin{equation*}
		\left\langle \tilde{e}_{k+1},e_i \right\rangle
		= \left\langle v_{k+1},e_i \right\rangle - \left\langle v_{k+1},e_i \right\rangle
		= 0 \text{ if } i\leq k.
	\end{equation*}
	Also $\tilde{e}_{k+1}\neq 0$, as if $\tilde{e}_{k+1}= 0$, then $v_{k+1}\in \left\langle e_1,\ldots,e_k \right\rangle = \left\langle v_1,\ldots,v_k \right\rangle$ which contradicts $v_1,\ldots,v_{k+1}$ linearly independent. %
	
	So put $e_{k+1} = \tilde{e}_{k+1}/\left\vert \tilde{e}_{k+1} \right\vert$, and then $e_1,\ldots,e_{k+1}$ are orthonormal, and $\left\langle e_1,\ldots,e_{k+1} \right\rangle = \left\langle v_1,\ldots,v_{k+1} \right\rangle$. %
\end{proof}

\vspace{3pt}
\begin{corollary}
	Any orthonormal set can be extended to an orthonormal basis. %
\end{corollary}

\begin{proof}
	Extend the orthonormal set to a basis; now the Gram-Schmidt algorithm doesn't change $v_1,\ldots,v_k$ if they are already orthonormal. %
\end{proof}

Recall that if $W\leq V$, $W^\perp = {}^\perp W = \left\{v\in V \mid \left\langle v,w \right\rangle=0 \;\forall w\in W \right\}$.

\begin{proposition}
	If $W\leq V$, $V$ an inner product space, then $W\oplus W^\perp = V$. %
\end{proposition}

\begin{proof}
	[Proof 1] If $\left\langle \cdot,\cdot \right\rangle$ is positive definite on $V$, then it is also positive definite on $W$, and thus $\eval[0]{\left\langle \cdot,\cdot \right\rangle}_{W}$ is non-degenerate. If $\F=\R$, then $\left\langle \cdot,\cdot \right\rangle$ is bilinear, and we've shown that $W\oplus W^\perp=V$ when the form $\left\langle \cdot,\cdot \right\rangle\mid_{W}$ is non-degenerate. If $\F=\C$, then exactly the same proof for sesquilinear forms shows the result. %
\end{proof}

\begin{proof}
	[Proof 2] Pick an orthonormal basis $w_1,\ldots,w_r$ for $W$, and extend it to an orthonormal basis for $V$, $w_1,\ldots,w_n$. %
	
	Now observe that $\left\langle w_{r+1},\ldots,w_n \right\rangle=W^\perp$. Proof $(\subseteq)$ is done. For $(\supseteq)$: if $\sum_{i=1}^n \lambda_i\,w_i \in W^\perp$, then take $\left\langle \cdot,w_i \right\rangle$, $i\leq r$, and we get $\lambda_i=0$ for $i\leq r$. So $V=W\oplus W^\perp$. %
\end{proof}

\subsubsection*{Geometric interpretation of the key step in the Gram-Schmidt algorithm} % (fold)
\label{ssub:geometric}

Let $V$ be an inner product space, with $W\leq V$ and $V=W\oplus W^\perp$. Define a map $\pi:V\to W$, the \emph{orthogonal projection onto $W$}, defined as follows: if $v\in V$, then write $v=w+w\p$, where $w\in W$ and $w\p\in W^\perp$ uniquely, and set $\pi(v)=w$.

This satisfies $\eval[0]{\pi}_{W}=\id :W\to W$, $\pi^2=\pi$ and $\pi$ linear.

\begin{proposition}
	If $W$ has an orthonormal basis $e_1,\ldots,e_k$ and $\pi:V\to W$ as above, then %
	\begin{enumerate}
		\shortskip
		\item $\pi(v) = \sum_{i=1}^k \left\langle v,e_i \right\rangle e_i$;
		\item $\pi(v)$ is the vector in $W$ closest to $v$; that is, $\left\vert v-\pi(v) \right\vert\leq \left\vert v-w \right\vert$ for all $w\in W$, with equality if and only if $w=\pi(v)$. %
	\end{enumerate}
\end{proposition}

\begin{proof}
\mbox{}
\begin{enumerate}
	\item If $v\in V$, then put $w=\sum_{i=1}^k \left\langle v,e_i \right\rangle e_i$, and $w\p=v-w$. So $w\in W$, and we want $w\p\in W^\perp$. But %
	\begin{equation*}
		\left\langle w\p,e_i \right\rangle = \left\langle v,e_i \right\rangle - \left\langle v,e_i \right\rangle = 0\text{ for all i},\; %
		1\leq i\leq k, %
	\end{equation*}
	so indeed we have $w\p\in W^\perp$, and $\pi(v)=w$ by definition.
	\item We have $v-\pi(v)\in W^\perp$, and if $w\in W$, $\pi(v)-w\in W$, then
	\begin{align*}
		\left\vert v-w \right\vert^2
		&= \left\vert \left( v-\pi (v) \right) + \left( \pi (v)-w \right) \right\vert^2 \\ %
		&= \left\vert v-\pi (v) \right\vert^2 + \left\vert \pi (v)-w \right\vert^2 + 2\Re \underbrace{\langle \underset{\in W^\perp}{v-\pi (v)}, %
		\underset{\in W}{\pi (v)-w}
		\rangle}_{=0},
	\end{align*}
	and so $\left\vert v-w \right\vert^2 \geq \left\vert v-\pi(v) \right\vert^2$, with equality if and only if $\left\vert \pi(v) - w \right\vert = 0$; that is, if $\pi(v)=w$. \qedhere %
\end{enumerate}
\end{proof}

% subsubsection geometric (end)

% subsection inner_product_spaces (end)

\subsection{Hermitian adjoints for inner products} % (fold)
\label{sub:hermitian_adjoints_for_inner_products}

Let $V$ and $W$ be inner product spaces over $F$ and $\alpha:V\to W$ a linear map.

\begin{proposition}
	There is a unique \emph{linear} map $\alpha^*:W\to V$ such that for all $v\in V$, $w\in W$, $\left\langle \alpha(v),w \right\rangle=\left\langle v,\alpha^*(w) \right\rangle$. This map is called the \emph{Hermitian adjoint}. %
	
	Moreover, if $e_1,\ldots,e_n$ is an orthonormal basis of $V$, and $\,f_1,\ldots,f_m$ is an orthonormal basis for $W$, and $A=(a_{ij})$ is the matrix of $\alpha$ with respect to these bases, then $\overline{A^\Trans}$ %
	% =A^\dagger$ 
	is the matrix of $\alpha^*$. %
\end{proposition}

\begin{proof}
	 If $\beta:W\to V$ is a linear map with matrix $B=(b_{ij})$, then %
	 \begin{equation*}
	 	\left\langle \alpha(v),w \right\rangle = \left\langle v, \beta(w) \right\rangle \text{ for all } v,w
	 \end{equation*}
	 if and only if
	 \begin{equation*}
	 	\left\langle \alpha(e_j),f_k \right\rangle = \left\langle e_j,\beta(f_k) \right\rangle \text{ for all } 0\leq j\leq n, 0\leq k\leq m. %
	 \end{equation*}
	 But we have
	 \begin{equation*}
	 	a_{kj}
		= \left\langle \textstyle\sum a_{ij}\,f_i, f_k \right\rangle
		= \left\langle \alpha(e_j),f_k \right\rangle
		= \left\langle e_j,\beta(f_k) \right\rangle
		= \left\langle e_j,\textstyle\sum b_{ik}\,e_i \right\rangle
		= \overline{b}_{jk},
	 \end{equation*}
	 that is, $B=\overline{A^\Trans}$. Now define $\alpha^*$ to be the map with matrix $\overline{A^\Trans}$. %
\end{proof}

\begin{exercise}
	If $\F=\R$, identify $V\isomto V^*$ by $v\mapsto \left\langle v,\cdot \right\rangle$, $W\isomto W^*$ by $w\mapsto \left\langle w,\cdot \right\rangle$, and then show that $\alpha^*$ is just the dual map. %
	
	More generally, if $\alpha:V\to W$ defines a linear map over $\F$, $\psi\in\Bil(V)$, $\psi\p\in\Bil(W)$, both non-degenerate, then you can define the adjoint by $\psi'(\alpha(v),w) = \psi(v,\alpha^*(w))$ for all $v\in V$, $w\in W$, and show that it is the dual map. \lecturemarker{22}{23 Nov} %
\end{exercise}

\begin{lemma}
\mbox{}
\begin{enumerate}
	\shortskip
	\item If $\alpha,\beta:V\to W$, then $(\alpha+\beta)^* = \alpha^* + \beta^*$.
	\item $\left( \lambda\alpha \right)^* = \overline{\lambda} \alpha^*$.
	\item $\alpha^{**} = \alpha$.
\end{enumerate}
\end{lemma}

\begin{proof*}
	Immediate from the properties of $A\to \overline{A^\Trans}$. %
\end{proof*}

\begin{definition}
	A map $\alpha:V\to V$ is self-adjoint if $\alpha=\alpha^*$. 
\end{definition}


If $v_1,\ldots,v_n$ is an orthonormal basis for $V$, and $A$ is the matrix of $\alpha$, then $\alpha$ is self-adjoint if and only if $A=\overline{A}^\Trans$. %
	
In short, if $\F=\R$, then $A$ is symmetric, and if $\F=\C$, then $A$ is Hermitian.

	\pagebreak

\begin{theorem}
	Let $\alpha:V\to V$ be self-adjoint. Then %
	\begin{enumerate}
		\shortskip
		\item All the eigenvalues of $\alpha$ are real.
		\item Eigenvectors with distinct eigenvalues are orthogonal.
		\item There exists an orthogonal basis of eigenvectors for $\alpha$. In particular, $\alpha$ is diagonalisable. %
	\end{enumerate}
\end{theorem}

\vspace{-3pt}

\begin{proof}
\mbox{}
\begin{enumerate}
	\item First assume $\F=\C$. If $\alpha v = \lambda v$ for a non-zero vector $v$ and $\lambda\iC$, then %
	\begin{equation*}
		\lambda\left\langle v,v \right\rangle
		= \left\langle \lambda v,v \right\rangle
		= \left\langle v,\alpha^* v \right\rangle
		= \left\langle v,\alpha v \right\rangle
		= \left\langle v,\lambda v \right\rangle
		= \overline{\lambda} \left\langle v,v \right\rangle,
	\end{equation*}
	as $\alpha$ is self-adjoint. Since $v\neq 0$, we have $\left\langle v,v \right\rangle\neq 0$ and thus $\lambda=\overline{\lambda}$. %
	
	If $\F=\R$, then let $A=A^\Trans$ be the matrix of $\alpha$; regard it as a matrix over $\C$, which is obviously Hermitian, and then the above shows that the eigenvalue for $A$ is real. %
	
\smallskip

	\begin{remark}
		This shows that we should introduce some notation so that we can phrase this argument without choosing a basis. Here is one way: let $V$ be a vector space over $\R$. Define a new vector space, $V_\C = V\oplus iV$, a new vector space over $\R$ of twice the dimension, and make it a complex vector space by saying that $i\left( v+iw \right)=(-w+iv)$, so $\dim_\R V = \dim_\C V_\C$. Now suppose the matrix of $\alpha:V\to V$ is $A$. Then show the matrix of $\alpha_\C :V_\C\to V_\C$ is also $A$, where $\alpha_\C(v+iw) = \alpha(v)+i\,\alpha(w)$. %
	\end{remark}
	
	Now we can phrase (i) of the proof using $V_\C$: show $\lambda\iR$ implies that we can choose a  $\lambda$-eigenvector $v\in V_\C$ to be in $V\subseteq V_\C$. %
	
	\item If $\alpha(v_i)=\lambda_i\,v_i$, $i=1,2$, where $v_i\neq 0$ and $\lambda_1\neq \lambda_2$, then %
	\begin{equation*}
		\lambda_1\left\langle v_1,v_2 \right\rangle
		= \left\langle \alpha v_1,v_2 \right\rangle
		= \left\langle v_1,\alpha v_2 \right\rangle
		= \overline{\lambda}_2 \left\langle v_1,v_2 \right\rangle,
	\end{equation*}
	as $\alpha=\alpha^*$, so if $\left\langle v_1,v_2 \right\rangle\neq 0$, then $\lambda_1=\overline{\lambda}_2 = \lambda_2$, a contradiction. %
	
	\item Induct on $\dim V$. The case $\dim V=1$ is clear, so assume $n=\dim V>1$. By (i), there is a real eigenvalue $\lambda$, and an eigenvector $v_1\in V$ such that $\alpha(v_1)=\lambda v_1$. %
	Thus $V=\left\langle v_1 \right\rangle \oplus \left\langle v_1 \right\rangle^\perp$ as $V$ is an inner product space. Now put $W=\left\langle v_1 \right\rangle^\perp$. %
	
	\textbf{Claim.} $\alpha (W)\subseteq W$; that is, if $\left\langle x,v_1 \right\rangle=0$, then $\left\langle \alpha(x),v_1 \right\rangle=0$. %
	
	\emph{Proof.} We have
	\begin{equation*}
		\left\langle \alpha(x),v_1 \right\rangle 
		= \left\langle x,\alpha^*(v_1) \right\rangle
		= \left\langle x,\alpha(v_1) \right\rangle
		= \overline{\lambda}\left\langle x,v_1 \right\rangle = 0.
	\end{equation*}
	Also, $\eval[0]{\alpha}_W:W\to W$ is self-adjoint, as $\left\langle \alpha(v),w \right\rangle = \left\langle v,\alpha(w) \right\rangle$ for all $v,w\in V$, and so this is also true for all $v,w\in W$. Hence by induction $W$ has an orthonormal basis $v_2,\ldots,v_n$, and so $\hat{v}_1,v_2,\ldots,v_n$ is an orthonormal basis for $V$. \qedhere %
\end{enumerate}
\end{proof}

	\pagebreak

\begin{definition}
	Let $V$ be an inner product space over $\C$. Then the \emph{group of isometries} of the form $\left\langle \cdot,\cdot \right\rangle$, denoted $U(V)$, is defined to be %
	\begin{align*}
		U(V) = \Isom(V)
		&= \left\{\alpha:V\to V \mid \left\langle \alpha(v),\alpha(w) \right\rangle = \left\langle v,w \right\rangle \;\forall v,w\in V \right\} \\ %
		&= \left\{\alpha\in\GL(V) \mid \left\langle \alpha(v),w\p \right\rangle = \left\langle v,\alpha^{-1} w\p \right\rangle \; \forall v,w\p\in V\right\}, %
		\intertext{putting $w\p=\alpha(w)$. Now we note that $\alpha:V\to V$ an isometry implies that $\alpha$ is an isomorphism. This is because $v\neq 0$ if and only if $\left\vert v \right\vert\neq 0$, and $\alpha$ is an isometry, so we have $\left\vert \alpha v \right\vert=\left\vert v \right\vert\neq 0$, and so $\alpha$ is injective.} %
		&= \left\{\alpha\in\GL(V) \mid \alpha^{-1} = \alpha^* \right\}.
	\end{align*}
	This is called the \emph{unitary group}.
\end{definition}

If $V=\Cn$, and $\left\langle \cdot,\cdot \right\rangle$ is the standard inner product $\left\langle x,y \right\rangle = \sum_i x_i\,\overline{y}_i$, then we write
\begin{equation*}
	U_n = U(n) = U(\Cn)
	= \left\{X\in GL_n(\C) \mid \overline{X}^\Trans \cdot X = I \right\}.
\end{equation*}

So an orthonormal basis (that is, a choice of isomorphism $V\isomto \Cn$) gives us an isomorphism $U(V) \isomto U_n$.

\begin{theorem}
	Let $V$ be an inner product space over $\C$, and $\alpha:V\to V$ an isometry; that is, $\alpha^*=\alpha^{-1}$, and $\alpha\in U(V)$. Then \label{thm:eive-isometry} %
	\begin{enumerate}
		\shortskip
		\item All eigenvalues $\lambda$ of $\alpha$ have $\left\vert \lambda \right\vert=1$; that is, they lie on the unit circle. %
		\item Eigenvectors with distinct eigenvalues are orthogonal.
		\item There exists an orthonormal basis of eigenvectors for $\alpha$; in particular $\alpha$ is diagonalisable. %
	\end{enumerate}
\end{theorem}

\vspace{3pt}
\begin{remark}
	If $V$ is an inner product space over $\R$, then $\Isom\left\langle \cdot,\cdot \right\rangle=O(V)$, the usual orthogonal group,  also denoted $O_n(\R)$.  If we choose an orthonormal basis for $V$, then $\alpha\in O(V)$ if $A$, the matrix of $\alpha$, has $A^\Trans A=I$. %
	
	Then this theorem applied to $A$ considered as a complex matrix shows that $A$ is diagonalisable over $\C$, but as all the eigenvalues of $A$ have $\left\vert \lambda \right\vert=1$, it is not diagonalisable over $\R$ unles the only eigenvalues are $\pm 1$. %
\end{remark}

\begin{example}
	The matrix
	\begin{equation*}
		\mat{\cos\theta & \sin\theta \\ - \sin\theta & \cos\theta} = A\in O(2) %
	\end{equation*}
	is diagonalisable over $\C$, and conjugate to
	\begin{equation*}
		\mat{e^{i\theta} \\ & e^{-i\theta}},
	\end{equation*}
	but not over $\R$, unless $\sin\theta=0$. %
\end{example}

\begin{proof}
\mbox{}
\begin{enumerate}
	\item If $\alpha(v)=\lambda v$, for $v$ non-zero, then
	\begin{equation*}
		\lambda\left\langle v,v \right\rangle
		= \left\langle \lambda v,v \right\rangle
		= \left\langle \alpha(v),v \right\rangle
		= \left\langle v,\alpha^*(v) \right\rangle
		= \left\langle v,\alpha^{-1}(v) \right\rangle
		= \left\langle v,\lambda^{-1} v \right\rangle
		= \overline{\lambda}^{-1} \left\langle v,v \right\rangle,
	\end{equation*}
	and so $\lambda=\overline{\lambda}^{-1}$ and $\lambda\overline{\lambda}=1$.
	\item If $\alpha(v_i) = \lambda_i\,v_i$, for $v$ non-zero and $\lambda_i\neq \lambda_j$:
	\begin{equation*}
		\lambda_i\left\langle v_i,v_j \right\rangle
		= \left\langle \alpha(v_i),v_j \right\rangle
		= \left\langle v_i,\alpha^{-1}(v_j) \right\rangle
		= \overline{\lambda}_j^{-1}\left\langle v_i,v_j \right\rangle
		= \lambda_j \left\langle v_i,v_j \right\rangle,
	\end{equation*}
	and so $\lambda_i\neq \lambda_j$ implies $\left\langle v_i,v_j \right\rangle=0$.
	
	\item Induct on $n=\dim V$. If $V$ is a vector space over $\C$, then a non-zero eigenvector $v_1$ exists with some eigenvalue $\lambda$, so $\alpha(v_1)=\lambda v_1$. %
	
	Put $W=\left\langle v_1 \right\rangle^\perp$, so $V=\left\langle v_1 \right\rangle\oplus W$, as $V$ is an inner product space. %
	
	\textbf{Claim.} $\alpha(W)\subseteq W$; that is, $\left\langle x,v_1 \right\rangle$ implies $\left\langle \alpha(x),v_1 \right\rangle = 0$. %
	
	\emph{Proof.} We have
	\begin{equation*}
		\left\langle \alpha(x),v_1 \right\rangle
		= \left\langle x,\alpha^{-1}(v_1) \right\rangle
                = \left\langle x,\lambda^{-1}(v_1) \right\rangle %
		= \overline{\lambda^{-1}}\left\langle x,v_1 \right\rangle = 0. %
	\end{equation*}
	Also, $\left\langle \alpha(v),\alpha(w) \right\rangle=\left\langle v,w \right\rangle$ for all $v,w\in V$ implies that this is true for all $v,w\in W$, so $\eval[0]{\alpha}_W$ is unitary; that is $(\eval[0]{\alpha}_W)^* = (\eval[0]{\alpha}_W)^{-1}$, so induction gives an orthonormal basis of $W$, namely $v_2,\ldots,v_n$ of eigenvectors for $\alpha$, and so $\hat{v}_1,v_2,\ldots,v_n$ is an orthonormal basis for $V$. \qedhere %
\end{enumerate}
\end{proof}
\vspace{3pt}
\begin{remark}
	The previous two theorems admit the following generalisation: define $\alpha:V\to V$ to be \emph{normal} if $\alpha\alpha^* = \alpha^*\alpha$; that is, if $\alpha$ and $\alpha^*$ commute. %
\end{remark}

\begin{theorem}
	If $\alpha$ is normal, then there is an orthonormal basis consisting of eigenvalues for $\alpha$. 
\end{theorem}
\begin{proof} Exercise!
\end{proof}

\lecturemarker{23}{26 Nov}
Recall that
\begin{enumerate}
	\item $\GL_n(\C)$ acts on $\Mat_n(\C)$ taking $(P,A) \mapsto PAP^{-1}$. %
	
	\emph{Interpretation:} a choice of basis of a vector space $V$ identifies $\Mat_n(\C) \cong \Lin(V,V)$, and a change of basis changes $A$ to $PAP^{-1}$. %
	
	\item $\GL_n(\C)$ acts on $\Mat_n(\C)$ taking $(P,A) \mapsto PA\overline{P}^\Trans$. %
	
	\emph{Interpretation:} a choice of basis of a vector space $V$ identifies $\Mat_n(\C)$ with sesquilinear forms. %
	
	A change of basis changes $A$ to $PA\overline{P}^\Trans$ (where $P$ is $\overline{Q}^{-1}$, if $Q$ is the change of basis matrix). %
\end{enumerate}
These are genuinely different; that is, the theory of linear maps and sesquilinear forms are different.

But we have $P\in U_n$ if and only if $\overline{P}^\Trans P=I$, and $P^{-1}=\overline{P}^\Trans$, and then these two actions coincide! This occurs if and only if the columns of $P$ are an orthonormal basis with respect to usual inner product on $\Cn$.

\begin{proposition}
\mbox{}
\begin{enumerate}
	\shortskip
	\item Let $A\in\Mat_n(\C)$ be Hermitian, so $\herm{A}=A$. Then there exists a $P\in U_n$ such that $PAP^{-1} = PA\overline{P}^\Trans$ is real and diagonal. %
	\item Let $A\in\Mat_n (\R)$ be symmetric, with $A^\Trans = A$. Then there exists a $P\in O_n(\R)$ such that $PAP^{-1}=PAP^\Trans$ is real and diagonal. %
\end{enumerate}
\end{proposition}

\begin{proof}
	Given $A\in\Mat_n(\F)$ (for $\F=\C$ or $\R$), the map $\alpha:\Fn\to\Fn$ taking $x\mapsto Ax$ is self-adjoint with respect to the standard inner product. By theorem~\ref{thm:eive-isometry}, there is an orthonormal basis of eigenvectors for $\alpha:\Fn\to\Fn$, that is, there are some $\lambda_1,\ldots,\lambda_n\iR$ such that $Av_i = \lambda_i v_i$. Then %
	\begin{equation*}
		A \mat{v_1 & \cdots & v_n}
		= \mat{\lambda_1 v_1 \cdots \lambda_n v_n}
		= \mat{v_1 & \cdots & v_n} \mat{\lambda_1 \\ & \ddots \\ & & \lambda_n}.
	\end{equation*}
	If we set $Q=\mat{v_1 \cdots v_n} \in \Mat_n(\F)$, then
	\begin{equation*}
		AQ = Q \mat{\lambda_1 \\ & \ddots \\ & & \lambda_n},
	\end{equation*}
	and $v_1,\ldots,v_n$ are an orthonormal basis if and only if $\herm{Q}=Q^{-1}$, so we put $P=Q^{-1}$ and get the result. %
\end{proof}

\begin{corollary}
	If $\psi$ is a Hermitian form on $V$ with matrix $A$, then the signature $\sign(\psi)$ is the number of positive eigenvalues of $A$ less the number of negative eigenvalues. 
\end{corollary}

\begin{proof}
	If the matrix $A$ is diagonal, then this is clear: rescale the basis vectors $v_i \mapsto v_i/\left\vert v_i \right\vert$, and the signature is the number of original diagonal entries which are positive, less the number which are negative. %
	
	Now for general $A$, the proposition shows that we can choose $P\in U_n$ such that $PAP^{-1} = PA\herm{P}$ is diagonal, and this represents the same form with respect to the new basis, but also has the same eigenvalues. %
\end{proof}

\begin{corollary}
	Both $\rk(\psi)$ and $\sign(\psi)$ can be read off the characteristic polynomialomial of any matrix $A$ for $\psi$. % 
\end{corollary}

\begin{exercise}
	Let $\psi:\Rn\times\Rn \to \R$ be $\psi(x,y)=x^\Trans Ay$, where %
	\begin{equation*}
		A = \mat{0 & 1 & \cdots & \cdots & 1 \\ 1 & 0 & 1 & \cdots & 1 \\ \vdots & 1 & \ddots & & \vdots \\ \vdots & & & \ddots & \vdots \\ 1 & 1 & \cdots & 1 & 0}. %
	\end{equation*}
	Show that $\chi_A(x)=\left( x+1 \right)^{n-1}\left( x-\left( n-1 \right) \right)$, so the signature is $2-n$ and the rank is $n$. %
	
\end{exercise}
	Another consequence of the proposition is the simultaneous diagonalisation of some bilinear forms. %

\begin{theorem}
	Let $V$ be a finite dimensional vector space over $\C$ (or $\R$), and $\varphi,\psi:V\times V\to\F$ be two Hermitian (symmetric) bilinear forms. %
	
	If $\varphi$ is positive definite, then there is some basis $v_1,\ldots,v_n$ of $V$ such that with respect to this basis, both forms $\varphi$ and $\psi$ are diagonal; that is, $\psi(v_i,v_j) = \varphi(v_i,v_j) = 0$ if $i\neq j$. %
\end{theorem}

\begin{proof}
	As $\varphi$ is positive definite, there exists an orthonormal basis for $\varphi$; that is, some $w_1,\ldots,w_n$ such that $\varphi(w_i,w_j)=\delta_{ij}$. %
	
	Now let $B$ be the matrix of $\psi$ with respect to this basis; that is, $b_{ij} = \psi(w_i,w_j) = \overline{b}_{ji} = \overline{\psi}(w_j,w_i)$, as $\psi$ is Hermitian. %
	
	By the proposition, there is some $P\in U_n$ (or $O_n(\R)$, if $V$ is over $\R$) such that %
	\begin{equation*}
		\herm{P} BP = D = \mat{\lambda_1 & & 0 \\ & \cdots \\ 0 & & \lambda_n}
	\end{equation*}
	is diagonal, for $\lambda_i\iR$, and now the matrix of $\varphi$ with respect to our new basis, is $\herm{P}IP=I$, also diagonal. %
\end{proof}

Now we ask what is the ``meaning'' of the diagonal entries $\lambda_1,\ldots,\lambda_n$?

If $\varphi,\psi: V\times V\to \F$ are any two bilinear/sesquilinear forms, then they determine (anti)-linear maps $V\to V^*$ taking $v\mapsto \varphi(\cdot,v)$ and $v\mapsto\psi(\cdot,v)$, and if $\varphi$ is a non-degenerate form, then the map $V\to V^*$ taking $v\mapsto \varphi(\cdot,v)$ is an (anti)-linear \emph{isomorphism}. So we can take its inverse, and compose with the map $V\to V^*$, $v \mapsto \psi(\cdot,v)$ to get a (linear!) map $V\to V$.
%, with $V\to V^*$ (under arrow $v\mapsto \psi(\cdot,v)$) added to $V^*\isomfrom V$ (under arrow $\varphi(\cdot,v)$) and 
Then $\lambda_1,\ldots,\lambda_n$ are the eigenvalues of this map.

\begin{exercise}
	If $\varphi,\psi$ are both \emph{not} positive definite, then need they be simultaneously diagonalisable? %
\end{exercise}

\begin{remark}
	In coordinates: if we choose any basis for $V$, let the matrix of $\varphi$ be $A$ and that for $\psi$ be $B$, with respect to this basis. Then  $A=Q^\Trans Q$ for some $Q\in\GL_n(\C)$ as $\varphi$ is positive definite, and then the above proof shows that %
	\begin{equation*}
		B=\overline{Q}^{-\Trans} \overline{P}^{-\Trans} DP^{-1} Q^{-1},
	\end{equation*}
	since $P^{-1}=\herm{P}$. Then
	\begin{align*}
		\det(D-xI)
		&= \det(Q^{-\Trans} \left( P^{-\Trans} DP - xQ^\Trans Q \right) Q^{-1}) \\
		&= \det A \det(B-xA),
	\end{align*}
	and the diagonal entries are the roots of the polynomial $\det(B-xA)$; that is, the roots of $\det(BA^{-1}-xI)$, as claimed. %
\end{remark}

\lecturemarker{24}{28 Nov}
Consider the relationship between $O_n(\R) \injto \GL_n(\R)$, $U_n \injto \GL_n(\C)$.

\begin{example}
	Take $n=1$. We have
	\begin{equation*}
		\textstyle\GL_1(\C)=\C^*
		\qquad \text{and} \qquad
		U_1=\left\{\lambda\iC : \left\vert \lambda \right\vert=1 \right\} = S^{\,1}
	\end{equation*}
	We have $\C^* = S^{\,1}\times \R_{>0}$, with $\lambda r\mapsfrom (\lambda,r)$.
	
	In $\R$, we have $\GL_1(\R) = \R^*$, $O_1(\R)=\left\{\pm 1\right\}$ and $\R^* = \left\{\pm 1\right\} \times \R_{>0}$. %
	
	For $n>1$, Gram-Schmidt orthonormalisation tells us the relation: define
	\begin{equation*}
		\cal{A} = \left\{\mat{\lambda_1 & & 0 \\ & \ddots \\ 0 & & \lambda_n} \big|
		\lambda_i\iR_{>0}\right\},
		\qquad
		N(\F) = \left\{\mat{1 & & * \\ & \ddots \\ 0 & & 1} \big| *\iF\right\},
	\end{equation*}
	where $\F=\R$ or $\C$. Then $\cal{A}$ as a set, is homeomorphic to $\Rn$, and $N(\F)$ as a set (not a group) is isomorphic to $\F^{\f{1}{2}\left( n-1 \right)n}$, so $\R^{n(n-1)/2}$ or $\C^{n(n-1)/2}$.
\end{example}

\begin{exercise}
	Show that
	\begin{equation*}
		\cal{A}\cdot N(\F) = \left\{\mat{\lambda_1 & * \\ & \ddots \\ 0 & & \lambda_n} \mid \lambda_i\iR_{>0}, *\iF\right\} %
	\end{equation*}
	is a group, $N(\F)$ is a normal subgroup and $A\cap N(\F)=\{I\}$. %
\end{exercise}

\begin{theorem}
	Any $A\in\GL_n(\C)$ can be written uniquely as $A=QR$, with $Q\in U_n$, $R\in \cal{A}\cdot N(\C)$. %
	
	Similarly, any $A\in\GL_n(\R)$ can be written uniquely as $A=QR$, with $Q\in O_n(\R)$, $R\in\cal{A}\cdot N(\R)$. %
\end{theorem}

\begin{example}
	$n=1$ is above. %
\end{example}

\begin{proof}
	This is just Gram-Schmidt. %
	
	Write $A=\mat{v_1 & \cdots & v_n}$, $v_i\in\Fn$ so $v_1,\ldots,v_n$ is a basis for $\Fn$. Now the Gram-Schmidt algorithm gives an orthonormal basis $e_1,\ldots,e_n$. Recall how it went: set %
	\begin{align*}
		\tilde{e}_1 &= v_1, \\
		\tilde{e}_2 &= v_2 - \f{\left\langle v_2,\tilde{e}_1 \right\rangle}{\left\langle \tilde{e}_1,\tilde{e}_1 \right\rangle} \cdot \tilde{e}_1, \\ %
		\tilde{e}_3 &= v_3 - \f{\left\langle v_3,\tilde{e}_2 \right\rangle}{\left\langle \tilde{e}_2,\tilde{e}_2 \right\rangle}\cdot \tilde{e}_2 - \f{\left\langle v_3,\tilde{e}_1 \right\rangle}{\left\langle \tilde{e}_1,\tilde{e}_1 \right\rangle} \cdot \tilde{e}_1 \\ %
		\vdots\; & \\
		\tilde{e}_n &= v_n - \sum_{i=1}^{n-1} \f{\left\langle v_n,\tilde{e}_i \right\rangle}{\tilde{e}_i,\tilde{e}_i}\cdot \tilde{e}_i, %
	\end{align*}
	so that $\tilde{e}_1,\ldots,\tilde{e}_n$ are orthogonal, and if we set $e_i=\tilde{e}_i/\left\vert \tilde{e}_i \right\vert$, then $e_1,\ldots,e_n$ are an orthonormal basis. So %
	\begin{align*}
		\tilde{e}_i
		&= v_i + \text{correction terms} \\
		&= v_i + \left\langle \tilde{e}_1,\ldots,\tilde{e}_{i-1} \right\rangle \\
		&= v_i + \left\langle v_1,\ldots,v_{i-1} \right\rangle,
	\end{align*}
	so we can write
	\begin{align*}
		\tilde{e}_1 &= v_1, \\
		\tilde{e}_2 &= v_2 + \left( * \right) v_1, \\
		\tilde{e}_3 &= v_3 + \left( * \right) v_2 + \left( * \right) v_1,
	\end{align*}
	that is,
	\begin{equation*}
		\mat{\tilde{e}_1 & \cdots & \tilde{e}_n} = \mat{v_1 & \cdots & v_n} \mat{1 & & * \\ & \ddots \\ 0 & & 1}, \text{ with } *\iF, %
	\end{equation*}
	and 
	\begin{equation*}
		\mat{\tilde{e}_1 & \cdots & \tilde{e}_n} \mat{\lambda_1 & & 0 \\ & \ddots \\ 0 & & \lambda_n} = \mat{e_1 & \cdots & e_n}, %
	\end{equation*}
	with $\lambda_i=1/\left\vert \tilde{e}_i \right\vert$. So if $Q=\mat{e_1 & \cdots & e_n}$, this is %
	\begin{equation*}
		Q = A \underbrace{\mat{1 & & * \\ & \ddots \\ & & 1} \mat{\lambda_1 & & 0 \\ & \ddots \\ 0 & & \lambda_n}}_{\text{call this $R^{-1}$}}. %
	\end{equation*}
	Thus $QR=A$, with $R\in \cal{A}\cdot N(\F)$, and $e_1,\ldots,e_n$ is an orthonormal basis if and only if $Q\in U_n$; that is, if $\herm{Q} Q=I$. %
	
	For uniqueness: if $QR=Q\p R\p$, then 
	\begin{equation*}
		\underset{\in U_n}{\left( Q\right)^{-1} Q} = \underset{\in \cal{A}\cdot N(\F)}{R\p R^{-1}}. %
	\end{equation*}
	So it is enough to show that if $X=\left( x_{ij} \right) \in \cal{A}\cdot N(\F) \cap U_n$, then $X=I$. But %
	\begin{equation*}
		X= \mat{x_{11} & & * \\ & \ddots \\ 0 & & x_{nn}},
	\end{equation*}
	and both the columns and the rows are orthonormal bases since $X\in U_n$. Since the columns are an orthonormal basis, $\left\vert x_{11}\right\vert=1 $ implies $x_{12}=x_{13} = \cdots = x_{1n} = 0$, as $\sum_{i=1}^n \left\vert x_{1i} \right\vert^2=1$. %
	
	Then $x_{11}\iR_{>0} \cap \left\{\lambda\iC \mid \left\vert \lambda \right\vert=1\right\}$ implies $x_{11}=1$, so %
	\begin{equation*}
		X = \mat{1 & 0 \\ 0 & \arrrmat{ \\ \phantom{0} & X\p & \phantom{0} \\ \mbox{}}},
	\end{equation*}
	with $X\p\in U_{n-1} \cap \cal{A}\cdot N(\F)$, so induction gives $X\p=I$.
%\todo{Blurgh?}
	
	\emph{Warning.} Notice that $U_n$ is a group, $\cal{A}\cdot N(\C)$ is a group, and if you want you can make $U_n\times\cal{A}\cdot N(\C)$ into a group by the direct product. But if you do this, then the map in the theorem is \emph{not} a group homomorphism. %
	
	The theorem says the map
	\begin{equation*}
		\fullfunction{\phi}{U_n\times \cal{A}\cdot N(\C)}{\GL_n(\C)}{(Q,R)}{QR}
	\end{equation*}
	is a \emph{bijection} of sets, not an isomorphism of groups. %

	This theorem tells us that the `shape' of the group $\GL_n(\C)$ and the shape of the group $U_n$ are the ``same'' -- one differs from another by the product of a space of the form $\C^k$, a vector space. You will learn in topology the precise words for this -- these two groups are \emph{homotopic} -- and you will learn later on that this means that many of their essential features are the same. %
	
	Finally (!!!), let's give another proof that every element of the unitary group is diagonalisable. We already know a very strong form of this. The following proof gives a weaker result, but gives it for a wider class of groups. It uses the same ideas as in in the above (probably cryptic) remark. %

	Consider the map
	\begin{equation*}
		\fullfunction
			{\theta}
			{\underset{=\C^{n^2}=\R^{2n^2}}{\Mat_n(\C)}}
			{\underset{=\C^{n^2}=\R^{2n^2}}{\Mat_n(\C)}}
			{A}
			{\overset{\;}{\overline{A}^{\Trans} A}}. %
	\end{equation*}
	This is a continuous map, and $\theta^{-1}(\{ I\})=U_n$, so as this is the inverse image of a closed set, it is a closed subset of $\C^{n^2}$. We also observe that  $\sum_j \left\vert a_{ij} \right\vert^2 = 1$ implies $U_n \subseteq \left\{(a_{ij}) \mid \left\vert a_{ij} \right\vert\leq 1 \right\}$ is a bounded set, so $U_n$ is a closed bounded subset of $\C^{n^2}$. Thus $U_n$ is a \emph{compact topological space}, and a group (a \emph{compact group}). %
\end{proof}

\begin{proposition}
	Let $G\leq \GL_n(\C)$ be a subgroup such that $G$ is also a closed bounded subset, that is, a \emph{compact subgroup of $\GL_n(\C)$}. Then if $g\in G$, then $g$ is diagonalisable as an element of $\GL_n(\C)$. That is, there is some $P\in\GL_n(\C)$ such that $PgP^{-1}$ is diagonal. %
\end{proposition}
\vspace{-6pt}
\begin{example}
	Any $g\in U_n$ is diagonalisable. %
\end{example}

\begin{proof}
	Consider the sequence of elements $1,g,g^2,g^3,\ldots$ in $G$. As $G$ is a closed bounded subset, it must have a convergent subsequence. %
	
	Let $P\in\GL_n(\C)$ such that $PgP^{-1}$ is in JNF.
	
	\textbf{Claim.} The sequence $a_1,a_2,\ldots,a_n$ in $\GL_n$ converges if and only if $Pa_1 P^{-1},P a_2 P^{-1},\ldots$ converges. %
	
	\emph{Proof of claim.} For fixed $P$, the map $A\mapsto PAP^{-1}$ is a continuous map on $\C^{n^2}$. This implies the claim, as the matrix coefficients are linear functions of the matrix coefficients on $A$. %
	
	If $PgP^{-1}$ has a Jordan block of size $a>1$,
	\begin{equation*}
		\mat{\lambda & 1 & 0 \\ & \ddots & 1 \\ 0 & & \lambda}=\left( \lambda I+J_a \right), \; \lambda\neq 0,
	\end{equation*}
	then
	\begin{align*}
		\left( \lambda I+J_a \right)^N
		&= \lambda^N I + N\lambda^{N-1} J_a + {N \choose 2} \lambda^{N-2} J_a^2 + \cdots \\
		&= \mat{\lambda^N & N{\lambda^{N-1}} & \\ & \ddots \\ & & \ddots & N\lambda^{N-1} \\ & & & \lambda^N}. %
	\end{align*}
	If $|\lambda| > 1$,  this has unbounded coefficients on the diagonal as $N \to \infty$;  if  $|\lambda| < 1$,  this has unbounded coefficients on the diagonal as $N \to - \infty$, contradicting the existance of a convergent subsequence. %

	So it must be that $|\lambda| =1 $. But now examine the entries just above the diagonal, and observe these are unbounded as $N \to \infty$, contradicting the existence of a convergent subsequence. %
\end{proof}

% subsection hermitian_adjoints_for_inner_products (end)
